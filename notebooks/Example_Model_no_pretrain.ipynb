{"cells":[{"cell_type":"markdown","metadata":{"id":"hlkOSSkZzxIG"},"source":["# Base pre-train and fine-tune models for SubtaskB"]},{"cell_type":"markdown","metadata":{"id":"SGmhiDzwzxIJ"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xcF8f3lzxIK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679320042134,"user_tz":0,"elapsed":24061,"user":{"displayName":"Norbert Slinko","userId":"03908713760848921802"}},"outputId":"bf105453-e934-4449-b563-5a0e5d15ebde"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount google drive to save models later\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"_wKc2vOvzxIL"},"source":["Clone git repo containing helper scripts\n","\n","`pat` is a personal access token in order to clone the private repo\n","\n","Create a personal access token here https://github.com/settings/tokens "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSQpqGds5qEx","outputId":"586f35cb-f552-49d1-c4d5-1d9f72c564dc","executionInfo":{"status":"ok","timestamp":1679320079088,"user_tz":0,"elapsed":31472,"user":{"displayName":"Norbert Slinko","userId":"03908713760848921802"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'com4520DarwinProject'...\n","remote: Enumerating objects: 7302, done.\u001b[K\n","remote: Counting objects: 100% (137/137), done.\u001b[K\n","remote: Compressing objects: 100% (86/86), done.\u001b[K\n","remote: Total 7302 (delta 52), reused 116 (delta 45), pack-reused 7165\u001b[K\n","Receiving objects: 100% (7302/7302), 40.18 MiB | 17.63 MiB/s, done.\n","Resolving deltas: 100% (711/711), done.\n","/content/com4520DarwinProject\n","Branch 'framework' set up to track remote branch 'framework' from 'origin'.\n","Switched to a new branch 'framework'\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets\n","  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers->-r requirements.txt (line 1)) (4.65.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers->-r requirements.txt (line 1)) (1.13.1+cu116)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers->-r requirements.txt (line 1)) (0.14.1+cu116)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers->-r requirements.txt (line 1)) (1.22.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers->-r requirements.txt (line 1)) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers->-r requirements.txt (line 1)) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers->-r requirements.txt (line 1)) (3.8.1)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.4.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 2)) (6.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 2)) (9.0.0)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 2)) (23.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 2)) (2.27.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 2)) (2023.3.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets->-r requirements.txt (line 2)) (1.4.4)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 3)) (3.10.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 3)) (2022.10.31)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (22.2.0)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (2.0.12)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers->-r requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers->-r requirements.txt (line 1)) (1.1.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers->-r requirements.txt (line 1)) (8.1.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 1)) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers->-r requirements.txt (line 1)) (8.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 2)) (1.15.0)\n","Building wheels for collected packages: sentence_transformers\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=353d1084ed2cb017583d99338fda3fe4eb14421d8a1d2fb9e37775a9e203679d\n","  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n","Successfully built sentence_transformers\n","Installing collected packages: tokenizers, sentencepiece, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, sentence_transformers, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 sentence_transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.27.1 xxhash-3.2.0 yarl-1.8.2\n"]}],"source":["pat = ''\n","with open('/content/drive/MyDrive/pat.txt', 'r') as f:\n","    pat = f.read().rstrip()\n","!git clone https://{pat}@github.com/agneknie/com4520DarwinProject.git\n","\n","# install requirements\n","%cd com4520DarwinProject\n","!git checkout framework\n","!pip install -r requirements.txt\n","import site\n","site.main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IM2SDRUVzxIM"},"outputs":[],"source":["import os\n","import sys\n","import numpy as np\n","import random\n","import torch\n","\n","from sentence_transformers import SentenceTransformer\n","\n","sys.path.append( '/content/com4520DarwinProject/src' )\n","from data.pre_train_dataset import make_pre_train_dataset\n","from models.pre_train_model import make_pre_train_model\n","from data.extract_idioms import extract_idioms\n","from evaluation.evaluate import get_dev_results, format_results, save_eval_output\n","from models.fine_tune_model import fine_tune_model\n","from evaluation.evaluate import get_dev_results, format_results, save_eval_output\n"]},{"cell_type":"markdown","metadata":{"id":"qTs8nrjnzxIN"},"source":["## Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15Fx7MPAzxIN"},"outputs":[],"source":["\n","base_path = os.path.join(os.getcwd())\n","subtask_b_dataset_path = os.path.join(base_path, 'data', 'datasets', 'SemEval_2022_Task2_SubTaskB')\n","drive_models_path = '/content/drive/Shareddrives/COM4520 Darwin Project - Team Quebec /Models/'\n","\n","languages = ['EN', 'PT']\n","tokenize_idioms = True\n","seed = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSvvxdzGzxIO"},"outputs":[],"source":["def set_seed(seed: int):\n","    \"\"\"\n","    Modified from : https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_utils.py\n","    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n","    installed).\n","    Args:\n","        seed (:obj:`int`): The seed to set.\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    \n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # ^^ safe to call this function even if cuda is not available\n","\n","    ## From https://pytorch.org/docs/stable/notes/randomness.html\n","    torch.backends.cudnn.benchmark = False\n","\n","    ## Might want to use the following, but set CUBLAS_WORKSPACE_CONFIG=:16:8\n","    # try : \n","    #   torch.use_deterministic_algorithms(True)\n","    # except AttributeError: \n","    #   torch.set_deterministic( True )\n","    \n","set_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"kas9i5muzxIU"},"source":["## Fine-tune model\n","\n","The pre-train model created above is fine-tuned on the training data supplied in this https://github.com/H-TayyarMadabushi/SemEval_2022_Task2-idiomaticity repo\n","\n","Multiple negatives ranking loss and triplet loss are used (same as this paper https://aclanthology.org/2022.semeval-1.26/ which was the 1st place fine-tune team).\n","This is different from the baseline which uses cosine similarity loss only."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smRLALT1zxIU","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1679320870963,"user_tz":0,"elapsed":2799,"user":{"displayName":"Norbert Slinko","userId":"03908713760848921802"}},"outputId":"7d3060e2-456e-41ed-eddb-5c0adcaa222d"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-9fac700addb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtask_b_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, 'TrainData', 'train_data.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model = fine_tune_model(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/com4520DarwinProject/src/models/fine_tune_model.py\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(model_path, output_path, train_file, tokenize_idioms, languages, dev_eval_path, batch_size, num_epochs, warmup, checkpoint_path, checkpoint_save_steps, transform)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_idioms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_idioms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mpositives_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositivesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/com4520DarwinProject/src/data/idiom_dataset.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(csv_file, tokenize_idioms, tokenize_idioms_ignore_case, transform, languages)\u001b[0m\n\u001b[1;32m     25\u001b[0m \"\"\"\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_idioms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize_idioms_ignore_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# break down the data into sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/com4520DarwinProject/src/data/util.py\u001b[0m in \u001b[0;36mload_csv\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Shareddrives/COM4520 Darwin Project - Team Quebec /Models/Data & Datasets/Datasets/Silver Dataset Final/silver_ds_1.csv'"]}],"source":["\n","num_epochs = 4\n","\n","model_path = drive_models_path + 'base_model_tokenized'\n","output_path = os.path.join(drive_models_path, 'fine_tune', 'dataset_baseline', 'enhancement_none', 'tokenized' if tokenize_idioms else 'not_tokenized', 'epochs_' + str(num_epochs), 'seed_' + str(seed))\n","train_file = os.path.join(subtask_b_dataset_path, 'TrainData', 'train_data.csv')\n","\n","model = fine_tune_model(\n","    model_path,\n","    output_path,\n","    subtask_b_dataset_path,\n","    tokenize_idioms=tokenize_idioms,\n","    languages=languages,\n","    num_epochs=num_epochs\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DLNUeYDzxIU"},"outputs":[],"source":["\n","dev_eval_path = os.path.join(subtask_b_dataset_path, 'EvaluationData')\n","results_file = os.path.join(base_path, 'dev.results.csv')\n","\n","results = get_dev_results(model, dev_eval_path, results_file, ['fine_tune'], languages, tokenize_idioms=tokenize_idioms)\n","\n","format_results(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNjAw-lCzxIV"},"outputs":[],"source":["results_file = os.path.join(base_path, 'eval.results.csv')\n","save_eval_output(model, dev_eval_path, results_file, ['fine_tune'], languages, tokenize_idioms=tokenize_idioms)"]},{"cell_type":"markdown","metadata":{"id":"i9Yc3JGxzxIV"},"source":["## Fine-tune model with dataset transform example\n","\n","Fine tune a model that passes in the MWE along with the sentence as a second input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zYBCivLazxIV"},"outputs":[],"source":["\n","model_path = drive_models_path + 'pre_train/tokenized/epochs_4'\n","# output_path = os.path.join(drive_models_path, 'fine_tune', 'dataset_baseline', 'enhancement_add_mwe', 'tokenized' if tokenize_idioms else 'not_tokenized', 'epochs_' + str(num_epochs), 'seed_' + str(seed))\n","output_path = os.path.join(base_path, 'models', 'add_mwe')\n","train_file = os.path.join(subtask_b_dataset_path, 'TrainData', 'train_data.csv')\n","\n","\n","def add_MWE(sentences, MWEs):\n","    return [sentence + '[SEP]' + mwe for (sentence, mwe) in zip(sentences, MWEs)]\n","\n","model = fine_tune_model(\n","    model_path,\n","    output_path,\n","    train_file,\n","    tokenize_idioms=tokenize_idioms,\n","    languages=languages,\n","    num_epochs=1,\n","    transform=add_MWE\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L68qvHqyzxIW"},"outputs":[],"source":["dev_eval_path = os.path.join(subtask_b_dataset_path, 'EvaluationData')\n","results_file = os.path.join(base_path, 'dev.results.csv')\n","\n","results = get_dev_results(model, dev_eval_path, results_file, ['fine_tune'], languages, tokenize_idioms=tokenize_idioms, transform=add_MWE)\n","\n","format_results(results)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"}}},"nbformat":4,"nbformat_minor":0}