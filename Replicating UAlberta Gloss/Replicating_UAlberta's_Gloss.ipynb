{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# About the model\n",
        "\n",
        "The model is adapted from the baseline, and uses the same gloss method as described in [UAlberta at SemEval 2022 Task 2: Leveraging Glosses and Translations for Multilingual Idiomaticity Detection](https://arxiv.org/abs/2205.14084).\n",
        "\n",
        "The input to mBERT is the target sentence, without context under the 'sentence1' header and the target MWE along with a max of 2 glosses for each word in the MWE, all as one string. For PT and GA the gloss is given using the translation of the words into English (using google translate). \n",
        "\n",
        "The model gets a 0.7059 macro-f1 score for zero shot dev data and 0.8385 macro-f1 score for one shot dev data. The model is trained for 9 epochs.\n",
        "\n",
        "Could still investigate the effect of: more epochs, more data, using glosses in the source language, using more/ less glosses, looking at different translations of a word rather than only using the best according to google, using other translators, using a different wordnet. "
      ],
      "metadata": {
        "id": "KfFaMwnEo0at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "eLHDjJ3StXUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XluqaTVJ4MTG",
        "outputId": "5cf9302e-5985-41f2-b4ea-a6e020b3ed92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SemEval_2022_Task2-idiomaticity' already exists and is not an empty directory.\n",
            "fatal: destination path 'AStitchInLanguageModels' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (4.11.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.3.2.post1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "# setup\n",
        "\n",
        "!git clone https://github.com/H-TayyarMadabushi/SemEval_2022_Task2-idiomaticity.git\n",
        "!git clone https://github.com/H-TayyarMadabushi/AStitchInLanguageModels.git\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install -U deep-translator\n",
        "\n",
        "import site\n",
        "site.main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def load_csv( path, delimiter=',' ) : \n",
        "    header = None\n",
        "    data   = list()\n",
        "    with open( path, encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader( csvfile, delimiter=delimiter ) \n",
        "        for row in reader : \n",
        "            if header is None : \n",
        "                header = row\n",
        "                continue\n",
        "            data.append( row ) \n",
        "    return header, data\n",
        "\n",
        "def write_csv( data, location ) : \n",
        "    with open( location, 'w', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer( csvfile ) \n",
        "        writer.writerows( data ) \n",
        "    print( \"Wrote {}\".format( location ) ) \n",
        "    return\n",
        "\n",
        "# split some text on spaces or hyphens\n",
        "def split(text):\n",
        "    return re.split(' |-', text)"
      ],
      "metadata": {
        "id": "II38nFG24ZP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "yIrGCN4QtvNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# translate\n",
        "\n",
        "from deep_translator import GoogleTranslator \n",
        "\n",
        "# store translations for every word inside an MWE from PT and GA\n",
        "def batch_translate(header, data, target='EN'):\n",
        "    print('Batch translating...')\n",
        "    translate = {}\n",
        "    for elem in data:\n",
        "        lang = elem[header.index('Language')]\n",
        "        MWE_split = split(elem[header.index('MWE')])\n",
        "        if lang != target:\n",
        "            if lang not in translate:\n",
        "                translate[lang] = []\n",
        "            for word in MWE_split:\n",
        "                if word not in translate[lang]:\n",
        "                    translate[lang].append(word)\n",
        "    for lang, words in translate.items():\n",
        "        translation = GoogleTranslator(lang.lower(), target.lower()).translate_batch(words)\n",
        "        translate[lang] = {words[i]: translation[i] for i in range(len(words))}\n",
        "    print('Finished translating')\n",
        "    return translate\n"
      ],
      "metadata": {
        "id": "LS-reM6MK7tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gloss \n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "\n",
        "# get the first n definitions of a word\n",
        "def get_gloss(word, n_gloss):\n",
        "    gloss = [synset.definition().capitalize() for synset in wn.synsets(word)][:n_gloss] # if synset.name().split('.')[0] == word][:n_gloss]\n",
        "    return gloss + [''] * (n_gloss - len(gloss))\n",
        "\n",
        "# get the first n definitions for a group of words\n",
        "def get_glosses(MWE, lang, translations, n_gloss):\n",
        "    MWE_split = split(MWE)\n",
        "    if lang != 'EN':\n",
        "        MWE_split = [translations[lang][word] for word in MWE_split]\n",
        "    return [get_gloss(word, n_gloss) for word in MWE_split]\n",
        "\n",
        "# convert a list of lists of strings into a single string, separated by '. '\n",
        "def collapse(glosses):\n",
        "    return '. '.join([item for sublist in glosses for item in sublist])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYzc1TFED899",
        "outputId": "1f370643-b33f-4132-aced-0b17f6bad593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess\n",
        "\n",
        "def _get_train_data( data_location, file_name, include_context, include_idiom, include_gloss, n_gloss=2 ) :\n",
        "    \n",
        "    file_name = os.path.join( data_location, file_name ) \n",
        "\n",
        "    header, data = load_csv( file_name )\n",
        "\n",
        "    translations = batch_translate(header, data, target='EN')\n",
        "\n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header.append('sentence2')\n",
        "        \n",
        "    # ['DataID', 'Language', 'MWE', 'Setting', 'Previous', 'Target', 'Next', 'Label']\n",
        "    out_data = list()\n",
        "    for elem in data :\n",
        "        label     = elem[ header.index( 'Label'  ) ]\n",
        "        sentence1 = elem[ header.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ header.index( 'Previous' ) ], elem[ header.index( 'Target' ) ], elem[ header.index( 'Next' ) ] ] )\n",
        "        \n",
        "        this_row = [ label, sentence1 ] \n",
        "        sentence2 = ''\n",
        "        if include_idiom :\n",
        "            sentence2 += elem[ header.index( 'MWE' ) ]\n",
        "        if include_gloss:\n",
        "            glosses = get_glosses(elem[header.index('MWE')], elem[header.index('Language')], translations, n_gloss=n_gloss)\n",
        "            glosses = collapse(glosses)\n",
        "            sentence2 += '.\\n' + glosses\n",
        "        if len(sentence2) > 0:\n",
        "            this_row.append(sentence2)\n",
        "\n",
        "        out_data.append( this_row )\n",
        "        assert len( out_header ) == len( this_row )\n",
        "    return [ out_header ] + out_data\n",
        "\n",
        "\n",
        "def _get_dev_eval_data( data_location, input_file_name, gold_file_name, include_context, include_idiom, include_gloss, n_gloss=2 ) :\n",
        "\n",
        "    input_headers, input_data = load_csv( os.path.join( data_location, input_file_name ) )\n",
        "    gold_header  = gold_data = None\n",
        "    if not gold_file_name is None : \n",
        "        gold_header  , gold_data  = load_csv( os.path.join( data_location, gold_file_name  ) )\n",
        "        assert len( input_data ) == len( gold_data )\n",
        "\n",
        "    translations = batch_translate(input_headers, input_data, target='EN')\n",
        "\n",
        "    # ['ID', 'Language', 'MWE', 'Previous', 'Target', 'Next']\n",
        "    # ['ID', 'DataID', 'Language', 'Label']\n",
        "    \n",
        "    out_header = [ 'label', 'sentence1' ]\n",
        "    if include_idiom :\n",
        "        out_header.append('sentence2')\n",
        "\n",
        "    out_data = list()\n",
        "    for index in range( len( input_data ) ) :\n",
        "        label = 1\n",
        "        if not gold_file_name is None : \n",
        "            this_input_id = input_data[ index ][ input_headers.index( 'ID' ) ]\n",
        "            this_gold_id  = gold_data [ index ][ gold_header  .index( 'ID' ) ]\n",
        "            assert this_input_id == this_gold_id\n",
        "            \n",
        "            label     = gold_data[ index ][ gold_header.index( 'Label'  ) ]\n",
        "            \n",
        "        elem      = input_data[ index ]\n",
        "        sentence1 = elem[ input_headers.index( 'Target' ) ]\n",
        "        if include_context :\n",
        "            sentence1 = ' '.join( [ elem[ input_headers.index( 'Previous' ) ], elem[ input_headers.index( 'Target' ) ], elem[ input_headers.index( 'Next' ) ] ] )\n",
        "        \n",
        "        this_row = [ label, sentence1 ] \n",
        "        sentence2 = ''\n",
        "        if include_idiom :\n",
        "            sentence2 += elem[ input_headers.index( 'MWE' ) ]\n",
        "        if include_gloss:\n",
        "            glosses = get_glosses(elem[input_headers.index('MWE')], elem[input_headers.index('Language')], translations, n_gloss=n_gloss)\n",
        "            glosses = collapse(glosses)\n",
        "            sentence2 += '\\n' + glosses\n",
        "        if len(sentence2) > 0:\n",
        "            this_row.append(sentence2)\n",
        "\n",
        "        assert len( out_header ) == len( this_row ) \n",
        "        out_data.append( this_row )\n",
        "        \n",
        "\n",
        "    return [ out_header ] + out_data\n",
        "\n",
        "\n",
        "def create_data(input_location, output_location):\n",
        "    # zero shot data\n",
        "    train_data = _get_train_data(\n",
        "            data_location   = input_location,\n",
        "            file_name       = 'train_zero_shot.csv',\n",
        "            include_context = False,\n",
        "            include_idiom   = True,\n",
        "            include_gloss   = True\n",
        "        )\n",
        "    write_csv( train_data, os.path.join( output_location, 'ZeroShot', 'train.csv' ) )\n",
        "\n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv', \n",
        "        include_context = False,\n",
        "        include_idiom   = True,\n",
        "        include_gloss   = True\n",
        "    )        \n",
        "    write_csv( dev_data, os.path.join( output_location, 'ZeroShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None , ## Don't have gold evaluation file -- submit to CodaLab\n",
        "        include_context = False,\n",
        "        include_idiom   = True,\n",
        "        include_gloss   = True\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'ZeroShot', 'eval.csv' ) )\n",
        "\n",
        "    # one shot data\n",
        "    train_zero_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_zero_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True,\n",
        "        include_gloss   = True\n",
        "    )\n",
        "    train_one_data = _get_train_data(\n",
        "        data_location   = input_location,\n",
        "        file_name       = 'train_one_shot.csv',\n",
        "        include_context = False,\n",
        "        include_idiom   = True,\n",
        "        include_gloss   = True\n",
        "    )\n",
        "\n",
        "    assert train_zero_data[0] == train_one_data[0] ## Headers\n",
        "    train_data = train_one_data + train_zero_data[1:]\n",
        "    write_csv( train_data, os.path.join( output_location, 'OneShot', 'train.csv' ) )\n",
        "\n",
        "\n",
        "    dev_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'dev.csv',\n",
        "        gold_file_name   = 'dev_gold.csv', \n",
        "        include_context  = False,\n",
        "        include_idiom    = True,\n",
        "        include_gloss   = True\n",
        "    )        \n",
        "    write_csv( dev_data, os.path.join( output_location, 'OneShot', 'dev.csv' ) )\n",
        "    \n",
        "    eval_data = _get_dev_eval_data(\n",
        "        data_location    = input_location,\n",
        "        input_file_name  = 'eval.csv',\n",
        "        gold_file_name   = None,\n",
        "        include_context  = False,\n",
        "        include_idiom    = True,\n",
        "        include_gloss   = True\n",
        "    )\n",
        "    write_csv( eval_data, os.path.join( output_location, 'OneShot', 'eval.csv' ) )"
      ],
      "metadata": {
        "id": "8wdN7D4k5mwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outpath = 'Data'\n",
        "\n",
        "Path( os.path.join( outpath, 'ZeroShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "Path( os.path.join( outpath, 'OneShot' ) ).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "create_data( 'SemEval_2022_Task2-idiomaticity/SubTaskA/Data/', outpath )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jqoQCVZ50_2",
        "outputId": "11bac2b2-b2f2-48f2-9c8d-6027c44643cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch translating...\n",
            "Finished translating\n",
            "Wrote Data/ZeroShot/train.csv\n",
            "Batch translating...\n",
            "Finished translating\n",
            "Wrote Data/ZeroShot/dev.csv\n",
            "Batch translating...\n",
            "Finished translating\n",
            "Wrote Data/ZeroShot/eval.csv\n",
            "Batch translating...\n",
            "Finished translating\n",
            "Batch translating...\n",
            "Finished translating\n",
            "Wrote Data/OneShot/train.csv\n",
            "Batch translating...\n",
            "Finished translating\n",
            "Wrote Data/OneShot/dev.csv\n",
            "Batch translating...\n",
            "Finished translating\n",
            "Wrote Data/OneShot/eval.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero Shot\n"
      ],
      "metadata": {
        "id": "nzCuM2YHlSdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "EWTc54iot09h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train zero shot\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osiyACnjZ2l_",
        "outputId": "3d13b5a7-9473-42bb-866a-792eec0fd006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/runs/Nov04_16-41-29_e5fee68aa035,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/ZeroShot/0/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: Data/ZeroShot/train.csv\n",
            "INFO:__main__:load a local file for validation: Data/ZeroShot/dev.csv\n",
            "WARNING:datasets.builder:Using custom data configuration default-717606636e138c78\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-717606636e138c78/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 115.40it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-11-04 16:41:29,998 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-04 16:41:30,001 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-04 16:41:30,029 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-04 16:41:30,030 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 16:41:30,031 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 16:41:30,031 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 16:41:30,031 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 16:41:30,031 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 16:41:30,031 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-04 16:41:30,031 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-04 16:41:30,032 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2158] 2022-11-04 16:41:30,169 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2599] 2022-11-04 16:41:33,017 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2611] 2022-11-04 16:41:33,017 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-717606636e138c78/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0c4aaba7345e2d86.arrow\n",
            "  0% 0/1 [00:00<?, ?ba/s]\n",
            "INFO:__main__:Sample 3155 of the training set: {'label': 0, 'sentence1': 'Caesar then created the leap year calendar to fix the problem, which was later adapted in accordance with new knowledge about the earth’s orbit by Pope Gregory into the Gregorian calendar that we observe today.', 'sentence2': 'leap year.\\nA light, self-propelled movement upwards or forwards. An abrupt transition. A period of time containing 365 (or 366) days. A period of time occupying a regular part of a calendar year that is used for some particular activity', 'input_ids': [101, 30159, 11059, 13745, 10105, 20169, 10410, 10924, 61637, 10114, 14045, 10686, 10105, 18077, 117, 10319, 10134, 10873, 40851, 10106, 88651, 10169, 10751, 22975, 10978, 10105, 39189, 100, 187, 17090, 10155, 23874, 22392, 10708, 10105, 47723, 11630, 61637, 10189, 11951, 78275, 18745, 119, 102, 20169, 10410, 10924, 119, 138, 15765, 117, 16567, 118, 30045, 14000, 10162, 17123, 10741, 30086, 10345, 23307, 10107, 119, 10313, 11357, 46791, 35959, 119, 138, 13127, 10108, 10635, 27248, 25385, 113, 10345, 35612, 114, 13990, 119, 138, 13127, 10108, 10635, 183, 104560, 10230, 169, 15670, 10668, 10108, 169, 61637, 10924, 10189, 10124, 11031, 10142, 11152, 15018, 22205, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 3445 of the training set: {'label': 1, 'sentence1': 'São Paulo – Alta comissária para os Direitos Humanos da Organização das Nações Unidas (ONU), a ex-presidenta do Chile Michelle Bachelet criticou governantes de países pobres e ricos que optaram pela economia em vez de promover a saúde da população.', 'sentence2': 'direitos humanos.\\nAn abstract idea of that which is due to a person or governmental body by law or tradition or nature; ; - eleanor roosevelt. Location near or direction toward the right side; i.e. the side to the south when a person or object faces east. All of the living human inhabitants of the earth. Any living or extinct member of the family hominidae characterized by superior intelligence, articulate speech, and erect carriage', 'input_ids': [101, 12114, 13360, 100, 22798, 10212, 47097, 19075, 10220, 10427, 66130, 10107, 87537, 10143, 72154, 27187, 10242, 95322, 49323, 113, 46743, 114, 117, 169, 11419, 118, 107041, 10149, 13218, 27062, 18965, 41583, 82389, 10138, 25574, 25819, 10104, 15395, 64440, 173, 99702, 10121, 10303, 35474, 10147, 11793, 28223, 10266, 11675, 10104, 57833, 169, 54728, 10143, 17857, 119, 102, 58475, 24806, 119, 10313, 66770, 14932, 10108, 10189, 10319, 10124, 10850, 10114, 169, 15042, 10345, 100092, 14333, 10155, 13255, 10345, 20049, 10345, 16613, 132, 132, 118, 12637, 12301, 10129, 25470, 14569, 107030, 119, 13069, 26287, 12883, 10345, 15599, 32216, 10105, 13448, 12250, 132, 177, 119, 173, 119, 10105, 12250, 10114, 10105, 13144, 10841, 169, 15042, 10345, 29331, 48343, 13827, 119, 11101, 10108, 10105, 14625, 14179, 28348, 10108, 10105, 39189, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.\n",
            "INFO:__main__:Sample 331 of the training set: {'label': 1, 'sentence1': 'On May 11th at the IFSEC Security Industry Awards 2009, Concept Smoke Screen were honoured with the \"Physical Security Product of the Year\" award, for the Guardian Smoke Screen.', 'sentence2': 'smoke screen.\\nA cloud of fine particles suspended in a gas. A hot vapor containing fine particles of carbon being produced by combustion. A white or silvered surface where pictures can be projected for viewing. A protective covering that keeps things out or hinders sight', 'input_ids': [101, 10576, 10725, 34062, 10160, 10105, 25000, 39039, 10858, 20924, 25539, 12357, 10195, 117, 77961, 80677, 41131, 10309, 46948, 10336, 10169, 10105, 107, 33671, 20924, 93218, 10108, 10105, 13567, 107, 17725, 117, 10142, 10105, 20206, 80677, 41131, 119, 102, 100332, 29963, 119, 138, 78394, 10108, 13435, 75593, 49799, 10106, 169, 16091, 119, 138, 29698, 48425, 27248, 13435, 75593, 10108, 36915, 11223, 13433, 10155, 10212, 96641, 119, 138, 15263, 10345, 23394, 10336, 16004, 10940, 54156, 10944, 10347, 52452, 10162, 10142, 17904, 10230, 119, 138, 32949, 11942, 41810, 10189, 107781, 24682, 10950, 10345, 19911, 25779, 78327, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:1930] 2022-11-04 16:41:35,280 >> Loading model from models/ZeroShot/0/checkpoint-141.\n",
            "[INFO|trainer.py:726] 2022-11-04 16:41:36,375 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1608] 2022-11-04 16:41:37,301 >> ***** Running training *****\n",
            "[INFO|trainer.py:1609] 2022-11-04 16:41:37,301 >>   Num examples = 4491\n",
            "[INFO|trainer.py:1610] 2022-11-04 16:41:37,301 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1611] 2022-11-04 16:41:37,301 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1612] 2022-11-04 16:41:37,301 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1613] 2022-11-04 16:41:37,301 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1614] 2022-11-04 16:41:37,302 >>   Total optimization steps = 1269\n",
            "[INFO|trainer.py:1616] 2022-11-04 16:41:37,302 >>   Number of trainable parameters = 177854978\n",
            "[INFO|trainer.py:1637] 2022-11-04 16:41:37,303 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1638] 2022-11-04 16:41:37,303 >>   Continuing training from epoch 1\n",
            "[INFO|trainer.py:1639] 2022-11-04 16:41:37,303 >>   Continuing training from global step 141\n",
            "[INFO|trainer.py:1642] 2022-11-04 16:41:37,303 >>   Will skip the first 1 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches: : 0it [00:00, ?it/s]\n",
            "Skipping the first batches: : 0it [00:00, ?it/s]\n",
            "\n",
            " 11% 142/1269 [00:01<00:12, 90.90it/s]\u001b[A\n",
            " 12% 152/1269 [00:07<01:14, 15.01it/s]\u001b[A\n",
            " 12% 156/1269 [00:10<01:44, 10.61it/s]\u001b[A\n",
            " 13% 159/1269 [00:12<02:14,  8.26it/s]\u001b[A\n",
            " 13% 161/1269 [00:13<02:39,  6.94it/s]\u001b[A\n",
            " 13% 162/1269 [00:14<02:56,  6.27it/s]\u001b[A\n",
            " 13% 163/1269 [00:14<03:18,  5.56it/s]\u001b[A\n",
            " 13% 164/1269 [00:15<03:47,  4.86it/s]\u001b[A\n",
            " 13% 165/1269 [00:15<04:21,  4.22it/s]\u001b[A\n",
            " 13% 166/1269 [00:16<05:04,  3.63it/s]\u001b[A\n",
            " 13% 167/1269 [00:17<05:51,  3.14it/s]\u001b[A\n",
            " 13% 168/1269 [00:17<06:41,  2.74it/s]\u001b[A\n",
            " 13% 169/1269 [00:18<07:31,  2.44it/s]\u001b[A\n",
            " 13% 170/1269 [00:18<08:19,  2.20it/s]\u001b[A\n",
            " 13% 171/1269 [00:19<09:02,  2.02it/s]\u001b[A\n",
            " 14% 172/1269 [00:20<09:36,  1.90it/s]\u001b[A\n",
            " 14% 173/1269 [00:20<10:05,  1.81it/s]\u001b[A\n",
            " 14% 174/1269 [00:21<10:28,  1.74it/s]\u001b[A\n",
            " 14% 175/1269 [00:22<10:45,  1.70it/s]\u001b[A\n",
            " 14% 176/1269 [00:22<10:55,  1.67it/s]\u001b[A\n",
            " 14% 177/1269 [00:23<11:05,  1.64it/s]\u001b[A\n",
            " 14% 178/1269 [00:24<11:09,  1.63it/s]\u001b[A\n",
            " 14% 179/1269 [00:24<11:14,  1.62it/s]\u001b[A\n",
            " 14% 180/1269 [00:25<11:17,  1.61it/s]\u001b[A\n",
            " 14% 181/1269 [00:25<11:18,  1.60it/s]\u001b[A\n",
            " 14% 182/1269 [00:26<11:19,  1.60it/s]\u001b[A\n",
            " 14% 183/1269 [00:27<11:19,  1.60it/s]\u001b[A\n",
            " 14% 184/1269 [00:27<11:20,  1.59it/s]\u001b[A\n",
            " 15% 185/1269 [00:28<11:20,  1.59it/s]\u001b[A\n",
            " 15% 186/1269 [00:29<11:20,  1.59it/s]\u001b[A\n",
            " 15% 187/1269 [00:29<11:19,  1.59it/s]\u001b[A\n",
            " 15% 188/1269 [00:30<11:20,  1.59it/s]\u001b[A\n",
            " 15% 189/1269 [00:30<11:20,  1.59it/s]\u001b[A\n",
            " 15% 190/1269 [00:31<11:20,  1.59it/s]\u001b[A\n",
            " 15% 191/1269 [00:32<11:19,  1.59it/s]\u001b[A\n",
            " 15% 192/1269 [00:32<11:20,  1.58it/s]\u001b[A\n",
            " 15% 193/1269 [00:33<11:18,  1.59it/s]\u001b[A\n",
            " 15% 194/1269 [00:34<11:19,  1.58it/s]\u001b[A\n",
            " 15% 195/1269 [00:34<11:17,  1.58it/s]\u001b[A\n",
            " 15% 196/1269 [00:35<11:16,  1.59it/s]\u001b[A\n",
            " 16% 197/1269 [00:35<11:15,  1.59it/s]\u001b[A\n",
            " 16% 198/1269 [00:36<11:14,  1.59it/s]\u001b[A\n",
            " 16% 199/1269 [00:37<11:14,  1.59it/s]\u001b[A\n",
            " 16% 200/1269 [00:37<11:16,  1.58it/s]\u001b[A\n",
            " 16% 201/1269 [00:38<11:16,  1.58it/s]\u001b[A\n",
            " 16% 202/1269 [00:39<11:16,  1.58it/s]\u001b[A\n",
            " 16% 203/1269 [00:39<11:14,  1.58it/s]\u001b[A\n",
            " 16% 204/1269 [00:40<11:14,  1.58it/s]\u001b[A\n",
            " 16% 205/1269 [00:41<11:14,  1.58it/s]\u001b[A\n",
            " 16% 206/1269 [00:41<11:16,  1.57it/s]\u001b[A\n",
            " 16% 207/1269 [00:42<11:14,  1.57it/s]\u001b[A\n",
            " 16% 208/1269 [00:42<11:15,  1.57it/s]\u001b[A\n",
            " 16% 209/1269 [00:43<11:15,  1.57it/s]\u001b[A\n",
            " 17% 210/1269 [00:44<11:15,  1.57it/s]\u001b[A\n",
            " 17% 211/1269 [00:44<11:15,  1.57it/s]\u001b[A\n",
            " 17% 212/1269 [00:45<11:16,  1.56it/s]\u001b[A\n",
            " 17% 213/1269 [00:46<11:15,  1.56it/s]\u001b[A\n",
            " 17% 214/1269 [00:46<11:14,  1.56it/s]\u001b[A\n",
            " 17% 215/1269 [00:47<11:14,  1.56it/s]\u001b[A\n",
            " 17% 216/1269 [00:48<11:13,  1.56it/s]\u001b[A\n",
            " 17% 217/1269 [00:48<11:12,  1.57it/s]\u001b[A\n",
            " 17% 218/1269 [00:49<11:14,  1.56it/s]\u001b[A\n",
            " 17% 219/1269 [00:50<11:12,  1.56it/s]\u001b[A\n",
            " 17% 220/1269 [00:50<11:13,  1.56it/s]\u001b[A\n",
            " 17% 221/1269 [00:51<11:13,  1.56it/s]\u001b[A\n",
            " 17% 222/1269 [00:51<11:13,  1.55it/s]\u001b[A\n",
            " 18% 223/1269 [00:52<11:11,  1.56it/s]\u001b[A\n",
            " 18% 224/1269 [00:53<11:13,  1.55it/s]\u001b[A\n",
            " 18% 225/1269 [00:53<11:12,  1.55it/s]\u001b[A\n",
            " 18% 226/1269 [00:54<11:12,  1.55it/s]\u001b[A\n",
            " 18% 227/1269 [00:55<11:12,  1.55it/s]\u001b[A\n",
            " 18% 228/1269 [00:55<11:11,  1.55it/s]\u001b[A\n",
            " 18% 229/1269 [00:56<11:08,  1.56it/s]\u001b[A\n",
            " 18% 230/1269 [00:57<11:07,  1.56it/s]\u001b[A\n",
            " 18% 231/1269 [00:57<11:06,  1.56it/s]\u001b[A\n",
            " 18% 232/1269 [00:58<11:05,  1.56it/s]\u001b[A\n",
            " 18% 233/1269 [00:59<11:04,  1.56it/s]\u001b[A\n",
            " 18% 234/1269 [00:59<11:05,  1.55it/s]\u001b[A\n",
            " 19% 235/1269 [01:00<11:06,  1.55it/s]\u001b[A\n",
            " 19% 236/1269 [01:00<11:06,  1.55it/s]\u001b[A\n",
            " 19% 237/1269 [01:01<11:05,  1.55it/s]\u001b[A\n",
            " 19% 238/1269 [01:02<11:05,  1.55it/s]\u001b[A\n",
            " 19% 239/1269 [01:02<11:04,  1.55it/s]\u001b[A\n",
            " 19% 240/1269 [01:03<11:03,  1.55it/s]\u001b[A\n",
            " 19% 241/1269 [01:04<11:03,  1.55it/s]\u001b[A\n",
            " 19% 242/1269 [01:04<11:03,  1.55it/s]\u001b[A\n",
            " 19% 243/1269 [01:05<11:03,  1.55it/s]\u001b[A\n",
            " 19% 244/1269 [01:06<11:02,  1.55it/s]\u001b[A\n",
            " 19% 245/1269 [01:06<11:01,  1.55it/s]\u001b[A\n",
            " 19% 246/1269 [01:07<11:01,  1.55it/s]\u001b[A\n",
            " 19% 247/1269 [01:08<11:01,  1.55it/s]\u001b[A\n",
            " 20% 248/1269 [01:08<10:59,  1.55it/s]\u001b[A\n",
            " 20% 249/1269 [01:09<10:58,  1.55it/s]\u001b[A\n",
            " 20% 250/1269 [01:10<10:58,  1.55it/s]\u001b[A\n",
            " 20% 251/1269 [01:10<11:00,  1.54it/s]\u001b[A\n",
            " 20% 252/1269 [01:11<11:00,  1.54it/s]\u001b[A\n",
            " 20% 253/1269 [01:11<11:03,  1.53it/s]\u001b[A\n",
            " 20% 254/1269 [01:12<11:06,  1.52it/s]\u001b[A\n",
            " 20% 255/1269 [01:13<11:06,  1.52it/s]\u001b[A\n",
            " 20% 256/1269 [01:13<11:06,  1.52it/s]\u001b[A\n",
            " 20% 257/1269 [01:14<11:05,  1.52it/s]\u001b[A\n",
            " 20% 258/1269 [01:15<11:05,  1.52it/s]\u001b[A\n",
            " 20% 259/1269 [01:15<11:06,  1.52it/s]\u001b[A\n",
            " 20% 260/1269 [01:16<11:04,  1.52it/s]\u001b[A\n",
            " 21% 261/1269 [01:17<11:03,  1.52it/s]\u001b[A\n",
            " 21% 262/1269 [01:17<11:05,  1.51it/s]\u001b[A\n",
            " 21% 263/1269 [01:18<11:03,  1.52it/s]\u001b[A\n",
            " 21% 264/1269 [01:19<11:01,  1.52it/s]\u001b[A\n",
            " 21% 265/1269 [01:19<10:56,  1.53it/s]\u001b[A\n",
            " 21% 266/1269 [01:20<10:54,  1.53it/s]\u001b[A\n",
            " 21% 267/1269 [01:21<10:53,  1.53it/s]\u001b[A\n",
            " 21% 268/1269 [01:21<10:52,  1.53it/s]\u001b[A\n",
            " 21% 269/1269 [01:22<10:50,  1.54it/s]\u001b[A\n",
            " 21% 270/1269 [01:23<10:49,  1.54it/s]\u001b[A\n",
            " 21% 271/1269 [01:23<10:48,  1.54it/s]\u001b[A\n",
            " 21% 272/1269 [01:24<10:48,  1.54it/s]\u001b[A\n",
            " 22% 273/1269 [01:25<10:47,  1.54it/s]\u001b[A\n",
            " 22% 274/1269 [01:25<10:46,  1.54it/s]\u001b[A\n",
            " 22% 275/1269 [01:26<10:47,  1.53it/s]\u001b[A\n",
            " 22% 276/1269 [01:27<10:47,  1.53it/s]\u001b[A\n",
            " 22% 277/1269 [01:27<10:47,  1.53it/s]\u001b[A\n",
            " 22% 278/1269 [01:28<10:46,  1.53it/s]\u001b[A\n",
            " 22% 279/1269 [01:29<10:47,  1.53it/s]\u001b[A\n",
            " 22% 280/1269 [01:29<10:46,  1.53it/s]\u001b[A\n",
            " 22% 281/1269 [01:30<10:47,  1.53it/s]\u001b[A\n",
            " 22% 282/1269 [01:30<08:55,  1.84it/s]\u001b[A[INFO|trainer.py:726] 2022-11-04 16:43:07,908 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:43:07,910 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:43:07,910 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:43:07,910 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.9461475610733032, 'eval_accuracy': 0.7374830842018127, 'eval_f1': 0.7362268177803944, 'eval_runtime': 5.3641, 'eval_samples_per_second': 137.767, 'eval_steps_per_second': 17.337, 'epoch': 2.0}\n",
            "100% 93/93 [00:05<00:00, 17.20it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:43:13,275 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-282\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:43:13,276 >> Configuration saved in models/ZeroShot/0/checkpoint-282/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:43:16,212 >> Model weights saved in models/ZeroShot/0/checkpoint-282/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:43:16,212 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-282/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:43:16,213 >> Special tokens file saved in models/ZeroShot/0/checkpoint-282/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 16:43:22,090 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-141] due to args.save_total_limit\n",
            "\n",
            " 22% 283/1269 [01:45<1:20:04,  4.87s/it]\u001b[A\n",
            " 22% 284/1269 [01:46<59:11,  3.61s/it]  \u001b[A\n",
            " 22% 285/1269 [01:46<44:35,  2.72s/it]\u001b[A\n",
            " 23% 286/1269 [01:47<34:23,  2.10s/it]\u001b[A\n",
            " 23% 287/1269 [01:48<27:14,  1.66s/it]\u001b[A\n",
            " 23% 288/1269 [01:48<22:15,  1.36s/it]\u001b[A\n",
            " 23% 289/1269 [01:49<18:42,  1.15s/it]\u001b[A\n",
            " 23% 290/1269 [01:50<16:18,  1.00it/s]\u001b[A\n",
            " 23% 291/1269 [01:50<14:34,  1.12it/s]\u001b[A\n",
            " 23% 292/1269 [01:51<13:21,  1.22it/s]\u001b[A\n",
            " 23% 293/1269 [01:52<12:31,  1.30it/s]\u001b[A\n",
            " 23% 294/1269 [01:52<11:54,  1.36it/s]\u001b[A\n",
            " 23% 295/1269 [01:53<11:30,  1.41it/s]\u001b[A\n",
            " 23% 296/1269 [01:54<11:13,  1.45it/s]\u001b[A\n",
            " 23% 297/1269 [01:54<11:01,  1.47it/s]\u001b[A\n",
            " 23% 298/1269 [01:55<10:52,  1.49it/s]\u001b[A\n",
            " 24% 299/1269 [01:55<10:45,  1.50it/s]\u001b[A\n",
            " 24% 300/1269 [01:56<10:41,  1.51it/s]\u001b[A\n",
            " 24% 301/1269 [01:57<10:38,  1.52it/s]\u001b[A\n",
            " 24% 302/1269 [01:57<10:39,  1.51it/s]\u001b[A\n",
            " 24% 303/1269 [01:58<10:43,  1.50it/s]\u001b[A\n",
            " 24% 304/1269 [01:59<10:44,  1.50it/s]\u001b[A\n",
            " 24% 305/1269 [01:59<10:43,  1.50it/s]\u001b[A\n",
            " 24% 306/1269 [02:00<10:41,  1.50it/s]\u001b[A\n",
            " 24% 307/1269 [02:01<10:40,  1.50it/s]\u001b[A\n",
            " 24% 308/1269 [02:01<10:38,  1.50it/s]\u001b[A\n",
            " 24% 309/1269 [02:02<10:40,  1.50it/s]\u001b[A\n",
            " 24% 310/1269 [02:03<10:39,  1.50it/s]\u001b[A\n",
            " 25% 311/1269 [02:03<10:38,  1.50it/s]\u001b[A\n",
            " 25% 312/1269 [02:04<10:35,  1.51it/s]\u001b[A\n",
            " 25% 313/1269 [02:05<10:30,  1.52it/s]\u001b[A\n",
            " 25% 314/1269 [02:05<10:28,  1.52it/s]\u001b[A\n",
            " 25% 315/1269 [02:06<10:27,  1.52it/s]\u001b[A\n",
            " 25% 316/1269 [02:07<10:25,  1.52it/s]\u001b[A\n",
            " 25% 317/1269 [02:07<10:25,  1.52it/s]\u001b[A\n",
            " 25% 318/1269 [02:08<10:24,  1.52it/s]\u001b[A\n",
            " 25% 319/1269 [02:09<10:22,  1.53it/s]\u001b[A\n",
            " 25% 320/1269 [02:09<10:22,  1.52it/s]\u001b[A\n",
            " 25% 321/1269 [02:10<10:21,  1.52it/s]\u001b[A\n",
            " 25% 322/1269 [02:11<10:21,  1.52it/s]\u001b[A\n",
            " 25% 323/1269 [02:11<10:19,  1.53it/s]\u001b[A\n",
            " 26% 324/1269 [02:12<10:19,  1.53it/s]\u001b[A\n",
            " 26% 325/1269 [02:13<10:19,  1.52it/s]\u001b[A\n",
            " 26% 326/1269 [02:13<10:20,  1.52it/s]\u001b[A\n",
            " 26% 327/1269 [02:14<10:19,  1.52it/s]\u001b[A\n",
            " 26% 328/1269 [02:15<10:22,  1.51it/s]\u001b[A\n",
            " 26% 329/1269 [02:15<10:23,  1.51it/s]\u001b[A\n",
            " 26% 330/1269 [02:16<10:23,  1.50it/s]\u001b[A\n",
            " 26% 331/1269 [02:17<10:21,  1.51it/s]\u001b[A\n",
            " 26% 332/1269 [02:17<10:21,  1.51it/s]\u001b[A\n",
            " 26% 333/1269 [02:18<10:21,  1.51it/s]\u001b[A\n",
            " 26% 334/1269 [02:19<10:21,  1.50it/s]\u001b[A\n",
            " 26% 335/1269 [02:19<10:19,  1.51it/s]\u001b[A\n",
            " 26% 336/1269 [02:20<10:18,  1.51it/s]\u001b[A\n",
            " 27% 337/1269 [02:21<10:18,  1.51it/s]\u001b[A\n",
            " 27% 338/1269 [02:21<10:19,  1.50it/s]\u001b[A\n",
            " 27% 339/1269 [02:22<10:18,  1.50it/s]\u001b[A\n",
            " 27% 340/1269 [02:23<10:15,  1.51it/s]\u001b[A\n",
            " 27% 341/1269 [02:23<10:16,  1.50it/s]\u001b[A\n",
            " 27% 342/1269 [02:24<10:17,  1.50it/s]\u001b[A\n",
            " 27% 343/1269 [02:25<10:17,  1.50it/s]\u001b[A\n",
            " 27% 344/1269 [02:25<10:16,  1.50it/s]\u001b[A\n",
            " 27% 345/1269 [02:26<10:17,  1.50it/s]\u001b[A\n",
            " 27% 346/1269 [02:27<10:16,  1.50it/s]\u001b[A\n",
            " 27% 347/1269 [02:27<10:15,  1.50it/s]\u001b[A\n",
            " 27% 348/1269 [02:28<10:16,  1.49it/s]\u001b[A\n",
            " 28% 349/1269 [02:29<10:15,  1.49it/s]\u001b[A\n",
            " 28% 350/1269 [02:29<10:15,  1.49it/s]\u001b[A\n",
            " 28% 351/1269 [02:30<10:14,  1.49it/s]\u001b[A\n",
            " 28% 352/1269 [02:31<10:12,  1.50it/s]\u001b[A\n",
            " 28% 353/1269 [02:31<10:13,  1.49it/s]\u001b[A\n",
            " 28% 354/1269 [02:32<10:11,  1.50it/s]\u001b[A\n",
            " 28% 355/1269 [02:33<10:11,  1.50it/s]\u001b[A\n",
            " 28% 356/1269 [02:33<10:10,  1.50it/s]\u001b[A\n",
            " 28% 357/1269 [02:34<10:11,  1.49it/s]\u001b[A\n",
            " 28% 358/1269 [02:35<10:09,  1.49it/s]\u001b[A\n",
            " 28% 359/1269 [02:35<10:08,  1.49it/s]\u001b[A\n",
            " 28% 360/1269 [02:36<10:08,  1.49it/s]\u001b[A\n",
            " 28% 361/1269 [02:37<10:07,  1.49it/s]\u001b[A\n",
            " 29% 362/1269 [02:37<10:07,  1.49it/s]\u001b[A\n",
            " 29% 363/1269 [02:38<10:05,  1.50it/s]\u001b[A\n",
            " 29% 364/1269 [02:39<10:05,  1.49it/s]\u001b[A\n",
            " 29% 365/1269 [02:39<10:04,  1.50it/s]\u001b[A\n",
            " 29% 366/1269 [02:40<10:04,  1.49it/s]\u001b[A\n",
            " 29% 367/1269 [02:41<10:03,  1.49it/s]\u001b[A\n",
            " 29% 368/1269 [02:41<10:03,  1.49it/s]\u001b[A\n",
            " 29% 369/1269 [02:42<10:02,  1.49it/s]\u001b[A\n",
            " 29% 370/1269 [02:43<10:01,  1.50it/s]\u001b[A\n",
            " 29% 371/1269 [02:43<10:00,  1.50it/s]\u001b[A\n",
            " 29% 372/1269 [02:44<09:58,  1.50it/s]\u001b[A\n",
            " 29% 373/1269 [02:45<09:56,  1.50it/s]\u001b[A\n",
            " 29% 374/1269 [02:45<09:55,  1.50it/s]\u001b[A\n",
            " 30% 375/1269 [02:46<09:55,  1.50it/s]\u001b[A\n",
            " 30% 376/1269 [02:47<09:55,  1.50it/s]\u001b[A\n",
            " 30% 377/1269 [02:47<09:54,  1.50it/s]\u001b[A\n",
            " 30% 378/1269 [02:48<09:53,  1.50it/s]\u001b[A\n",
            " 30% 379/1269 [02:49<09:53,  1.50it/s]\u001b[A\n",
            " 30% 380/1269 [02:49<09:52,  1.50it/s]\u001b[A\n",
            " 30% 381/1269 [02:50<09:53,  1.50it/s]\u001b[A\n",
            " 30% 382/1269 [02:51<09:52,  1.50it/s]\u001b[A\n",
            " 30% 383/1269 [02:51<09:52,  1.50it/s]\u001b[A\n",
            " 30% 384/1269 [02:52<09:53,  1.49it/s]\u001b[A\n",
            " 30% 385/1269 [02:53<09:53,  1.49it/s]\u001b[A\n",
            " 30% 386/1269 [02:53<09:52,  1.49it/s]\u001b[A\n",
            " 30% 387/1269 [02:54<09:50,  1.49it/s]\u001b[A\n",
            " 31% 388/1269 [02:55<09:50,  1.49it/s]\u001b[A\n",
            " 31% 389/1269 [02:55<09:49,  1.49it/s]\u001b[A\n",
            " 31% 390/1269 [02:56<09:48,  1.49it/s]\u001b[A\n",
            " 31% 391/1269 [02:57<09:47,  1.49it/s]\u001b[A\n",
            " 31% 392/1269 [02:57<09:47,  1.49it/s]\u001b[A\n",
            " 31% 393/1269 [02:58<09:46,  1.49it/s]\u001b[A\n",
            " 31% 394/1269 [02:59<09:46,  1.49it/s]\u001b[A\n",
            " 31% 395/1269 [02:59<09:45,  1.49it/s]\u001b[A\n",
            " 31% 396/1269 [03:00<09:45,  1.49it/s]\u001b[A\n",
            " 31% 397/1269 [03:01<09:44,  1.49it/s]\u001b[A\n",
            " 31% 398/1269 [03:01<09:42,  1.50it/s]\u001b[A\n",
            " 31% 399/1269 [03:02<09:45,  1.49it/s]\u001b[A\n",
            " 32% 400/1269 [03:03<09:45,  1.48it/s]\u001b[A\n",
            " 32% 401/1269 [03:03<09:44,  1.48it/s]\u001b[A\n",
            " 32% 402/1269 [03:04<09:45,  1.48it/s]\u001b[A\n",
            " 32% 403/1269 [03:05<09:45,  1.48it/s]\u001b[A\n",
            " 32% 404/1269 [03:05<09:45,  1.48it/s]\u001b[A\n",
            " 32% 405/1269 [03:06<09:45,  1.47it/s]\u001b[A\n",
            " 32% 406/1269 [03:07<09:46,  1.47it/s]\u001b[A\n",
            " 32% 407/1269 [03:08<09:46,  1.47it/s]\u001b[A\n",
            " 32% 408/1269 [03:08<09:45,  1.47it/s]\u001b[A\n",
            " 32% 409/1269 [03:09<09:46,  1.47it/s]\u001b[A\n",
            " 32% 410/1269 [03:10<09:44,  1.47it/s]\u001b[A\n",
            " 32% 411/1269 [03:10<09:43,  1.47it/s]\u001b[A\n",
            " 32% 412/1269 [03:11<09:44,  1.47it/s]\u001b[A\n",
            " 33% 413/1269 [03:12<09:44,  1.46it/s]\u001b[A\n",
            " 33% 414/1269 [03:12<09:43,  1.47it/s]\u001b[A\n",
            " 33% 415/1269 [03:13<09:41,  1.47it/s]\u001b[A\n",
            " 33% 416/1269 [03:14<09:42,  1.47it/s]\u001b[A\n",
            " 33% 417/1269 [03:14<09:41,  1.47it/s]\u001b[A\n",
            " 33% 418/1269 [03:15<09:40,  1.47it/s]\u001b[A\n",
            " 33% 419/1269 [03:16<09:40,  1.47it/s]\u001b[A\n",
            " 33% 420/1269 [03:16<09:38,  1.47it/s]\u001b[A\n",
            " 33% 421/1269 [03:17<09:39,  1.46it/s]\u001b[A\n",
            " 33% 422/1269 [03:18<09:40,  1.46it/s]\u001b[A\n",
            " 33% 423/1269 [03:18<07:59,  1.76it/s]\u001b[A[INFO|trainer.py:726] 2022-11-04 16:44:55,850 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:44:55,852 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:44:55,852 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:44:55,852 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3650379180908203, 'eval_accuracy': 0.7063599228858948, 'eval_f1': 0.7056614509246089, 'eval_runtime': 5.5876, 'eval_samples_per_second': 132.257, 'eval_steps_per_second': 16.644, 'epoch': 3.0}\n",
            "100% 93/93 [00:05<00:00, 16.51it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:45:01,441 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-423\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:45:01,442 >> Configuration saved in models/ZeroShot/0/checkpoint-423/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:45:03,962 >> Model weights saved in models/ZeroShot/0/checkpoint-423/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:45:03,962 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-423/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:45:03,963 >> Special tokens file saved in models/ZeroShot/0/checkpoint-423/special_tokens_map.json\n",
            "\n",
            " 33% 424/1269 [03:32<1:06:41,  4.74s/it]\u001b[A\n",
            " 33% 425/1269 [03:33<49:23,  3.51s/it]  \u001b[A\n",
            " 34% 426/1269 [03:34<37:20,  2.66s/it]\u001b[A\n",
            " 34% 427/1269 [03:34<28:53,  2.06s/it]\u001b[A\n",
            " 34% 428/1269 [03:35<23:00,  1.64s/it]\u001b[A\n",
            " 34% 429/1269 [03:36<18:53,  1.35s/it]\u001b[A\n",
            " 34% 430/1269 [03:36<16:02,  1.15s/it]\u001b[A\n",
            " 34% 431/1269 [03:37<14:03,  1.01s/it]\u001b[A\n",
            " 34% 432/1269 [03:38<12:40,  1.10it/s]\u001b[A\n",
            " 34% 433/1269 [03:39<11:41,  1.19it/s]\u001b[A\n",
            " 34% 434/1269 [03:39<11:00,  1.27it/s]\u001b[A\n",
            " 34% 435/1269 [03:40<10:29,  1.33it/s]\u001b[A\n",
            " 34% 436/1269 [03:41<10:06,  1.37it/s]\u001b[A\n",
            " 34% 437/1269 [03:41<09:50,  1.41it/s]\u001b[A\n",
            " 35% 438/1269 [03:42<09:40,  1.43it/s]\u001b[A\n",
            " 35% 439/1269 [03:43<09:32,  1.45it/s]\u001b[A\n",
            " 35% 440/1269 [03:43<09:26,  1.46it/s]\u001b[A\n",
            " 35% 441/1269 [03:44<09:22,  1.47it/s]\u001b[A\n",
            " 35% 442/1269 [03:45<09:19,  1.48it/s]\u001b[A\n",
            " 35% 443/1269 [03:45<09:18,  1.48it/s]\u001b[A\n",
            " 35% 444/1269 [03:46<09:18,  1.48it/s]\u001b[A\n",
            " 35% 445/1269 [03:47<09:16,  1.48it/s]\u001b[A\n",
            " 35% 446/1269 [03:47<09:14,  1.48it/s]\u001b[A\n",
            " 35% 447/1269 [03:48<09:14,  1.48it/s]\u001b[A\n",
            " 35% 448/1269 [03:49<09:14,  1.48it/s]\u001b[A\n",
            " 35% 449/1269 [03:49<09:14,  1.48it/s]\u001b[A\n",
            " 35% 450/1269 [03:50<09:15,  1.47it/s]\u001b[A\n",
            " 36% 451/1269 [03:51<09:13,  1.48it/s]\u001b[A\n",
            " 36% 452/1269 [03:51<09:14,  1.47it/s]\u001b[A\n",
            " 36% 453/1269 [03:52<09:14,  1.47it/s]\u001b[A\n",
            " 36% 454/1269 [03:53<09:13,  1.47it/s]\u001b[A\n",
            " 36% 455/1269 [03:53<09:11,  1.47it/s]\u001b[A\n",
            " 36% 456/1269 [03:54<09:12,  1.47it/s]\u001b[A\n",
            " 36% 457/1269 [03:55<09:11,  1.47it/s]\u001b[A\n",
            " 36% 458/1269 [03:55<09:10,  1.47it/s]\u001b[A\n",
            " 36% 459/1269 [03:56<09:12,  1.47it/s]\u001b[A\n",
            " 36% 460/1269 [03:57<09:12,  1.46it/s]\u001b[A\n",
            " 36% 461/1269 [03:57<09:11,  1.46it/s]\u001b[A\n",
            " 36% 462/1269 [03:58<09:11,  1.46it/s]\u001b[A\n",
            " 36% 463/1269 [03:59<09:10,  1.46it/s]\u001b[A\n",
            " 37% 464/1269 [04:00<09:11,  1.46it/s]\u001b[A\n",
            " 37% 465/1269 [04:00<09:08,  1.46it/s]\u001b[A\n",
            " 37% 466/1269 [04:01<09:08,  1.46it/s]\u001b[A\n",
            " 37% 467/1269 [04:02<09:08,  1.46it/s]\u001b[A\n",
            " 37% 468/1269 [04:02<09:06,  1.47it/s]\u001b[A\n",
            " 37% 469/1269 [04:03<09:06,  1.46it/s]\u001b[A\n",
            " 37% 470/1269 [04:04<09:06,  1.46it/s]\u001b[A\n",
            " 37% 471/1269 [04:04<09:03,  1.47it/s]\u001b[A\n",
            " 37% 472/1269 [04:05<09:04,  1.46it/s]\u001b[A\n",
            " 37% 473/1269 [04:06<09:03,  1.46it/s]\u001b[A\n",
            " 37% 474/1269 [04:06<09:01,  1.47it/s]\u001b[A\n",
            " 37% 475/1269 [04:07<09:00,  1.47it/s]\u001b[A\n",
            " 38% 476/1269 [04:08<08:58,  1.47it/s]\u001b[A\n",
            " 38% 477/1269 [04:08<08:57,  1.47it/s]\u001b[A\n",
            " 38% 478/1269 [04:09<08:57,  1.47it/s]\u001b[A\n",
            " 38% 479/1269 [04:10<08:55,  1.48it/s]\u001b[A\n",
            " 38% 480/1269 [04:10<08:54,  1.47it/s]\u001b[A\n",
            " 38% 481/1269 [04:11<08:54,  1.47it/s]\u001b[A\n",
            " 38% 482/1269 [04:12<08:52,  1.48it/s]\u001b[A\n",
            " 38% 483/1269 [04:12<08:51,  1.48it/s]\u001b[A\n",
            " 38% 484/1269 [04:13<08:51,  1.48it/s]\u001b[A\n",
            " 38% 485/1269 [04:14<08:50,  1.48it/s]\u001b[A\n",
            " 38% 486/1269 [04:14<08:50,  1.48it/s]\u001b[A\n",
            " 38% 487/1269 [04:15<08:50,  1.48it/s]\u001b[A\n",
            " 38% 488/1269 [04:16<08:50,  1.47it/s]\u001b[A\n",
            " 39% 489/1269 [04:16<08:49,  1.47it/s]\u001b[A\n",
            " 39% 490/1269 [04:17<08:48,  1.48it/s]\u001b[A\n",
            " 39% 491/1269 [04:18<08:46,  1.48it/s]\u001b[A\n",
            " 39% 492/1269 [04:19<08:46,  1.48it/s]\u001b[A\n",
            " 39% 493/1269 [04:19<08:44,  1.48it/s]\u001b[A\n",
            " 39% 494/1269 [04:20<08:43,  1.48it/s]\u001b[A\n",
            " 39% 495/1269 [04:21<08:43,  1.48it/s]\u001b[A\n",
            " 39% 496/1269 [04:21<08:40,  1.48it/s]\u001b[A\n",
            " 39% 497/1269 [04:22<08:41,  1.48it/s]\u001b[A\n",
            " 39% 498/1269 [04:23<08:40,  1.48it/s]\u001b[A\n",
            " 39% 499/1269 [04:23<08:39,  1.48it/s]\u001b[A\n",
            " 39% 500/1269 [04:24<08:39,  1.48it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.1154, 'learning_rate': 1.2119779353821908e-05, 'epoch': 3.55}\n",
            "\n",
            " 39% 500/1269 [04:24<08:39,  1.48it/s]\u001b[A\n",
            " 39% 501/1269 [04:25<08:39,  1.48it/s]\u001b[A\n",
            " 40% 502/1269 [04:25<08:38,  1.48it/s]\u001b[A\n",
            " 40% 503/1269 [04:26<08:37,  1.48it/s]\u001b[A\n",
            " 40% 504/1269 [04:27<08:37,  1.48it/s]\u001b[A\n",
            " 40% 505/1269 [04:27<08:38,  1.47it/s]\u001b[A\n",
            " 40% 506/1269 [04:28<08:37,  1.47it/s]\u001b[A\n",
            " 40% 507/1269 [04:29<08:37,  1.47it/s]\u001b[A\n",
            " 40% 508/1269 [04:29<08:35,  1.48it/s]\u001b[A\n",
            " 40% 509/1269 [04:30<08:36,  1.47it/s]\u001b[A\n",
            " 40% 510/1269 [04:31<08:35,  1.47it/s]\u001b[A\n",
            " 40% 511/1269 [04:31<08:34,  1.47it/s]\u001b[A\n",
            " 40% 512/1269 [04:32<08:33,  1.47it/s]\u001b[A\n",
            " 40% 513/1269 [04:33<08:33,  1.47it/s]\u001b[A\n",
            " 41% 514/1269 [04:33<08:35,  1.47it/s]\u001b[A\n",
            " 41% 515/1269 [04:34<08:33,  1.47it/s]\u001b[A\n",
            " 41% 516/1269 [04:35<08:32,  1.47it/s]\u001b[A\n",
            " 41% 517/1269 [04:35<08:30,  1.47it/s]\u001b[A\n",
            " 41% 518/1269 [04:36<08:30,  1.47it/s]\u001b[A\n",
            " 41% 519/1269 [04:37<08:29,  1.47it/s]\u001b[A\n",
            " 41% 520/1269 [04:38<08:29,  1.47it/s]\u001b[A\n",
            " 41% 521/1269 [04:38<08:30,  1.47it/s]\u001b[A\n",
            " 41% 522/1269 [04:39<08:30,  1.46it/s]\u001b[A\n",
            " 41% 523/1269 [04:40<08:29,  1.47it/s]\u001b[A\n",
            " 41% 524/1269 [04:40<08:28,  1.47it/s]\u001b[A\n",
            " 41% 525/1269 [04:41<08:28,  1.46it/s]\u001b[A\n",
            " 41% 526/1269 [04:42<08:28,  1.46it/s]\u001b[A\n",
            " 42% 527/1269 [04:42<08:27,  1.46it/s]\u001b[A\n",
            " 42% 528/1269 [04:43<08:25,  1.47it/s]\u001b[A\n",
            " 42% 529/1269 [04:44<08:24,  1.47it/s]\u001b[A\n",
            " 42% 530/1269 [04:44<08:23,  1.47it/s]\u001b[A\n",
            " 42% 531/1269 [04:45<08:24,  1.46it/s]\u001b[A\n",
            " 42% 532/1269 [04:46<08:24,  1.46it/s]\u001b[A\n",
            " 42% 533/1269 [04:46<08:24,  1.46it/s]\u001b[A\n",
            " 42% 534/1269 [04:47<08:25,  1.45it/s]\u001b[A\n",
            " 42% 535/1269 [04:48<08:24,  1.45it/s]\u001b[A\n",
            " 42% 536/1269 [04:48<08:24,  1.45it/s]\u001b[A\n",
            " 42% 537/1269 [04:49<08:24,  1.45it/s]\u001b[A\n",
            " 42% 538/1269 [04:50<08:23,  1.45it/s]\u001b[A\n",
            " 42% 539/1269 [04:51<08:23,  1.45it/s]\u001b[A\n",
            " 43% 540/1269 [04:51<08:21,  1.45it/s]\u001b[A\n",
            " 43% 541/1269 [04:52<08:20,  1.45it/s]\u001b[A\n",
            " 43% 542/1269 [04:53<08:20,  1.45it/s]\u001b[A\n",
            " 43% 543/1269 [04:53<08:19,  1.45it/s]\u001b[A\n",
            " 43% 544/1269 [04:54<08:19,  1.45it/s]\u001b[A\n",
            " 43% 545/1269 [04:55<08:18,  1.45it/s]\u001b[A\n",
            " 43% 546/1269 [04:55<08:17,  1.45it/s]\u001b[A\n",
            " 43% 547/1269 [04:56<08:16,  1.45it/s]\u001b[A\n",
            " 43% 548/1269 [04:57<08:15,  1.46it/s]\u001b[A\n",
            " 43% 549/1269 [04:57<08:13,  1.46it/s]\u001b[A\n",
            " 43% 550/1269 [04:58<08:13,  1.46it/s]\u001b[A\n",
            " 43% 551/1269 [04:59<08:13,  1.46it/s]\u001b[A\n",
            " 43% 552/1269 [04:59<08:12,  1.45it/s]\u001b[A\n",
            " 44% 553/1269 [05:00<08:12,  1.45it/s]\u001b[A\n",
            " 44% 554/1269 [05:01<08:12,  1.45it/s]\u001b[A\n",
            " 44% 555/1269 [05:02<08:10,  1.46it/s]\u001b[A\n",
            " 44% 556/1269 [05:02<08:10,  1.45it/s]\u001b[A\n",
            " 44% 557/1269 [05:03<08:09,  1.45it/s]\u001b[A\n",
            " 44% 558/1269 [05:04<08:08,  1.46it/s]\u001b[A\n",
            " 44% 559/1269 [05:04<08:07,  1.46it/s]\u001b[A\n",
            " 44% 560/1269 [05:05<08:05,  1.46it/s]\u001b[A\n",
            " 44% 561/1269 [05:06<08:06,  1.46it/s]\u001b[A\n",
            " 44% 562/1269 [05:06<08:05,  1.46it/s]\u001b[A\n",
            " 44% 563/1269 [05:07<08:04,  1.46it/s]\u001b[A\n",
            " 44% 564/1269 [05:07<06:39,  1.77it/s]\u001b[A[INFO|trainer.py:726] 2022-11-04 16:46:45,128 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:46:45,130 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:46:45,130 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:46:45,130 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.5888144969940186, 'eval_accuracy': 0.7090663313865662, 'eval_f1': 0.7088530586142003, 'eval_runtime': 5.6333, 'eval_samples_per_second': 131.183, 'eval_steps_per_second': 16.509, 'epoch': 4.0}\n",
            "100% 93/93 [00:05<00:00, 16.43it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:46:50,765 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-564\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:46:50,766 >> Configuration saved in models/ZeroShot/0/checkpoint-564/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:46:53,312 >> Model weights saved in models/ZeroShot/0/checkpoint-564/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:46:53,313 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-564/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:46:53,313 >> Special tokens file saved in models/ZeroShot/0/checkpoint-564/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 16:46:59,376 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-423] due to args.save_total_limit\n",
            "\n",
            " 45% 565/1269 [05:22<57:53,  4.93s/it]\u001b[A\n",
            " 45% 566/1269 [05:23<42:48,  3.65s/it]\u001b[A\n",
            " 45% 567/1269 [05:24<32:17,  2.76s/it]\u001b[A\n",
            " 45% 568/1269 [05:24<24:54,  2.13s/it]\u001b[A\n",
            " 45% 569/1269 [05:25<19:45,  1.69s/it]\u001b[A\n",
            " 45% 570/1269 [05:26<16:09,  1.39s/it]\u001b[A\n",
            " 45% 571/1269 [05:26<13:37,  1.17s/it]\u001b[A\n",
            " 45% 572/1269 [05:27<11:51,  1.02s/it]\u001b[A\n",
            " 45% 573/1269 [05:28<10:41,  1.08it/s]\u001b[A\n",
            " 45% 574/1269 [05:28<09:49,  1.18it/s]\u001b[A\n",
            " 45% 575/1269 [05:29<09:13,  1.25it/s]\u001b[A\n",
            " 45% 576/1269 [05:30<08:45,  1.32it/s]\u001b[A\n",
            " 45% 577/1269 [05:31<08:28,  1.36it/s]\u001b[A\n",
            " 46% 578/1269 [05:31<08:15,  1.40it/s]\u001b[A\n",
            " 46% 579/1269 [05:32<08:05,  1.42it/s]\u001b[A\n",
            " 46% 580/1269 [05:33<07:58,  1.44it/s]\u001b[A\n",
            " 46% 581/1269 [05:33<07:54,  1.45it/s]\u001b[A\n",
            " 46% 582/1269 [05:34<07:50,  1.46it/s]\u001b[A\n",
            " 46% 583/1269 [05:35<07:48,  1.47it/s]\u001b[A\n",
            " 46% 584/1269 [05:35<07:46,  1.47it/s]\u001b[A\n",
            " 46% 585/1269 [05:36<07:45,  1.47it/s]\u001b[A\n",
            " 46% 586/1269 [05:37<07:44,  1.47it/s]\u001b[A\n",
            " 46% 587/1269 [05:37<07:43,  1.47it/s]\u001b[A\n",
            " 46% 588/1269 [05:38<07:43,  1.47it/s]\u001b[A\n",
            " 46% 589/1269 [05:39<07:41,  1.47it/s]\u001b[A\n",
            " 46% 590/1269 [05:39<07:41,  1.47it/s]\u001b[A\n",
            " 47% 591/1269 [05:40<07:40,  1.47it/s]\u001b[A\n",
            " 47% 592/1269 [05:41<07:39,  1.47it/s]\u001b[A\n",
            " 47% 593/1269 [05:41<07:38,  1.47it/s]\u001b[A\n",
            " 47% 594/1269 [05:42<07:39,  1.47it/s]\u001b[A\n",
            " 47% 595/1269 [05:43<07:39,  1.47it/s]\u001b[A\n",
            " 47% 596/1269 [05:43<07:38,  1.47it/s]\u001b[A\n",
            " 47% 597/1269 [05:44<07:37,  1.47it/s]\u001b[A\n",
            " 47% 598/1269 [05:45<07:37,  1.47it/s]\u001b[A\n",
            " 47% 599/1269 [05:45<07:36,  1.47it/s]\u001b[A\n",
            " 47% 600/1269 [05:46<07:36,  1.47it/s]\u001b[A\n",
            " 47% 601/1269 [05:47<07:36,  1.46it/s]\u001b[A\n",
            " 47% 602/1269 [05:47<07:34,  1.47it/s]\u001b[A\n",
            " 48% 603/1269 [05:48<07:34,  1.47it/s]\u001b[A\n",
            " 48% 604/1269 [05:49<07:34,  1.46it/s]\u001b[A\n",
            " 48% 605/1269 [05:50<07:35,  1.46it/s]\u001b[A\n",
            " 48% 606/1269 [05:50<07:35,  1.45it/s]\u001b[A\n",
            " 48% 607/1269 [05:51<07:34,  1.46it/s]\u001b[A\n",
            " 48% 608/1269 [05:52<07:33,  1.46it/s]\u001b[A\n",
            " 48% 609/1269 [05:52<07:31,  1.46it/s]\u001b[A\n",
            " 48% 610/1269 [05:53<07:30,  1.46it/s]\u001b[A\n",
            " 48% 611/1269 [05:54<07:30,  1.46it/s]\u001b[A\n",
            " 48% 612/1269 [05:54<07:30,  1.46it/s]\u001b[A\n",
            " 48% 613/1269 [05:55<07:30,  1.46it/s]\u001b[A\n",
            " 48% 614/1269 [05:56<07:28,  1.46it/s]\u001b[A\n",
            " 48% 615/1269 [05:56<07:27,  1.46it/s]\u001b[A\n",
            " 49% 616/1269 [05:57<07:27,  1.46it/s]\u001b[A\n",
            " 49% 617/1269 [05:58<07:27,  1.46it/s]\u001b[A\n",
            " 49% 618/1269 [05:58<07:26,  1.46it/s]\u001b[A\n",
            " 49% 619/1269 [05:59<07:26,  1.46it/s]\u001b[A\n",
            " 49% 620/1269 [06:00<07:23,  1.46it/s]\u001b[A\n",
            " 49% 621/1269 [06:01<07:23,  1.46it/s]\u001b[A\n",
            " 49% 622/1269 [06:01<07:23,  1.46it/s]\u001b[A\n",
            " 49% 623/1269 [06:02<07:22,  1.46it/s]\u001b[A\n",
            " 49% 624/1269 [06:03<07:22,  1.46it/s]\u001b[A\n",
            " 49% 625/1269 [06:03<07:22,  1.46it/s]\u001b[A\n",
            " 49% 626/1269 [06:04<07:20,  1.46it/s]\u001b[A\n",
            " 49% 627/1269 [06:05<07:19,  1.46it/s]\u001b[A\n",
            " 49% 628/1269 [06:05<07:19,  1.46it/s]\u001b[A\n",
            " 50% 629/1269 [06:06<07:18,  1.46it/s]\u001b[A\n",
            " 50% 630/1269 [06:07<07:19,  1.45it/s]\u001b[A\n",
            " 50% 631/1269 [06:07<07:19,  1.45it/s]\u001b[A\n",
            " 50% 632/1269 [06:08<07:18,  1.45it/s]\u001b[A\n",
            " 50% 633/1269 [06:09<07:18,  1.45it/s]\u001b[A\n",
            " 50% 634/1269 [06:09<07:18,  1.45it/s]\u001b[A\n",
            " 50% 635/1269 [06:10<07:15,  1.45it/s]\u001b[A\n",
            " 50% 636/1269 [06:11<07:15,  1.45it/s]\u001b[A\n",
            " 50% 637/1269 [06:12<07:14,  1.45it/s]\u001b[A\n",
            " 50% 638/1269 [06:12<07:14,  1.45it/s]\u001b[A\n",
            " 50% 639/1269 [06:13<07:14,  1.45it/s]\u001b[A\n",
            " 50% 640/1269 [06:14<07:12,  1.45it/s]\u001b[A\n",
            " 51% 641/1269 [06:14<07:13,  1.45it/s]\u001b[A\n",
            " 51% 642/1269 [06:15<07:11,  1.45it/s]\u001b[A\n",
            " 51% 643/1269 [06:16<07:11,  1.45it/s]\u001b[A\n",
            " 51% 644/1269 [06:16<07:10,  1.45it/s]\u001b[A\n",
            " 51% 645/1269 [06:17<07:10,  1.45it/s]\u001b[A\n",
            " 51% 646/1269 [06:18<07:09,  1.45it/s]\u001b[A\n",
            " 51% 647/1269 [06:18<07:08,  1.45it/s]\u001b[A\n",
            " 51% 648/1269 [06:19<07:08,  1.45it/s]\u001b[A\n",
            " 51% 649/1269 [06:20<07:07,  1.45it/s]\u001b[A\n",
            " 51% 650/1269 [06:20<07:06,  1.45it/s]\u001b[A\n",
            " 51% 651/1269 [06:21<07:06,  1.45it/s]\u001b[A\n",
            " 51% 652/1269 [06:22<07:04,  1.45it/s]\u001b[A\n",
            " 51% 653/1269 [06:23<07:02,  1.46it/s]\u001b[A\n",
            " 52% 654/1269 [06:23<07:02,  1.46it/s]\u001b[A\n",
            " 52% 655/1269 [06:24<07:01,  1.46it/s]\u001b[A\n",
            " 52% 656/1269 [06:25<07:00,  1.46it/s]\u001b[A\n",
            " 52% 657/1269 [06:25<07:00,  1.46it/s]\u001b[A\n",
            " 52% 658/1269 [06:26<06:58,  1.46it/s]\u001b[A\n",
            " 52% 659/1269 [06:27<06:57,  1.46it/s]\u001b[A\n",
            " 52% 660/1269 [06:27<06:57,  1.46it/s]\u001b[A\n",
            " 52% 661/1269 [06:28<06:55,  1.46it/s]\u001b[A\n",
            " 52% 662/1269 [06:29<06:55,  1.46it/s]\u001b[A\n",
            " 52% 663/1269 [06:29<06:54,  1.46it/s]\u001b[A\n",
            " 52% 664/1269 [06:30<06:53,  1.46it/s]\u001b[A\n",
            " 52% 665/1269 [06:31<06:52,  1.46it/s]\u001b[A\n",
            " 52% 666/1269 [06:31<06:52,  1.46it/s]\u001b[A\n",
            " 53% 667/1269 [06:32<06:51,  1.46it/s]\u001b[A\n",
            " 53% 668/1269 [06:33<06:50,  1.46it/s]\u001b[A\n",
            " 53% 669/1269 [06:33<06:50,  1.46it/s]\u001b[A\n",
            " 53% 670/1269 [06:34<06:50,  1.46it/s]\u001b[A\n",
            " 53% 671/1269 [06:35<06:49,  1.46it/s]\u001b[A\n",
            " 53% 672/1269 [06:36<06:47,  1.47it/s]\u001b[A\n",
            " 53% 673/1269 [06:36<06:46,  1.46it/s]\u001b[A\n",
            " 53% 674/1269 [06:37<06:46,  1.47it/s]\u001b[A\n",
            " 53% 675/1269 [06:38<06:44,  1.47it/s]\u001b[A\n",
            " 53% 676/1269 [06:38<06:44,  1.47it/s]\u001b[A\n",
            " 53% 677/1269 [06:39<06:43,  1.47it/s]\u001b[A\n",
            " 53% 678/1269 [06:40<06:42,  1.47it/s]\u001b[A\n",
            " 54% 679/1269 [06:40<06:41,  1.47it/s]\u001b[A\n",
            " 54% 680/1269 [06:41<06:40,  1.47it/s]\u001b[A\n",
            " 54% 681/1269 [06:42<06:40,  1.47it/s]\u001b[A\n",
            " 54% 682/1269 [06:42<06:40,  1.47it/s]\u001b[A\n",
            " 54% 683/1269 [06:43<06:38,  1.47it/s]\u001b[A\n",
            " 54% 684/1269 [06:44<06:39,  1.46it/s]\u001b[A\n",
            " 54% 685/1269 [06:44<06:37,  1.47it/s]\u001b[A\n",
            " 54% 686/1269 [06:45<06:37,  1.47it/s]\u001b[A\n",
            " 54% 687/1269 [06:46<06:37,  1.46it/s]\u001b[A\n",
            " 54% 688/1269 [06:46<06:35,  1.47it/s]\u001b[A\n",
            " 54% 689/1269 [06:47<06:35,  1.47it/s]\u001b[A\n",
            " 54% 690/1269 [06:48<06:35,  1.46it/s]\u001b[A\n",
            " 54% 691/1269 [06:48<06:33,  1.47it/s]\u001b[A\n",
            " 55% 692/1269 [06:49<06:33,  1.47it/s]\u001b[A\n",
            " 55% 693/1269 [06:50<06:34,  1.46it/s]\u001b[A\n",
            " 55% 694/1269 [06:51<06:36,  1.45it/s]\u001b[A\n",
            " 55% 695/1269 [06:51<06:36,  1.45it/s]\u001b[A\n",
            " 55% 696/1269 [06:52<06:36,  1.45it/s]\u001b[A\n",
            " 55% 697/1269 [06:53<06:35,  1.45it/s]\u001b[A\n",
            " 55% 698/1269 [06:53<06:36,  1.44it/s]\u001b[A\n",
            " 55% 699/1269 [06:54<06:36,  1.44it/s]\u001b[A\n",
            " 55% 700/1269 [06:55<06:34,  1.44it/s]\u001b[A\n",
            " 55% 701/1269 [06:55<06:33,  1.44it/s]\u001b[A\n",
            " 55% 702/1269 [06:56<06:31,  1.45it/s]\u001b[A\n",
            " 55% 703/1269 [06:57<06:29,  1.45it/s]\u001b[A\n",
            " 55% 704/1269 [06:57<06:28,  1.46it/s]\u001b[A\n",
            " 56% 705/1269 [06:58<05:20,  1.76it/s]\u001b[A[INFO|trainer.py:726] 2022-11-04 16:48:35,572 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:48:35,574 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:48:35,574 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:48:35,574 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.970557451248169, 'eval_accuracy': 0.7009472250938416, 'eval_f1': 0.7005172948103648, 'eval_runtime': 5.5971, 'eval_samples_per_second': 132.032, 'eval_steps_per_second': 16.616, 'epoch': 5.0}\n",
            "100% 93/93 [00:05<00:00, 16.58it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:48:41,172 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-705\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:48:41,173 >> Configuration saved in models/ZeroShot/0/checkpoint-705/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:48:43,776 >> Model weights saved in models/ZeroShot/0/checkpoint-705/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:48:43,777 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-705/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:48:43,777 >> Special tokens file saved in models/ZeroShot/0/checkpoint-705/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 16:48:49,792 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-564] due to args.save_total_limit\n",
            "\n",
            " 56% 706/1269 [07:13<46:21,  4.94s/it]\u001b[A\n",
            " 56% 707/1269 [07:14<34:15,  3.66s/it]\u001b[A\n",
            " 56% 708/1269 [07:14<25:50,  2.76s/it]\u001b[A\n",
            " 56% 709/1269 [07:15<19:56,  2.14s/it]\u001b[A\n",
            " 56% 710/1269 [07:16<15:48,  1.70s/it]\u001b[A\n",
            " 56% 711/1269 [07:16<12:56,  1.39s/it]\u001b[A\n",
            " 56% 712/1269 [07:17<10:54,  1.18s/it]\u001b[A\n",
            " 56% 713/1269 [07:18<09:30,  1.03s/it]\u001b[A\n",
            " 56% 714/1269 [07:18<08:30,  1.09it/s]\u001b[A\n",
            " 56% 715/1269 [07:19<07:48,  1.18it/s]\u001b[A\n",
            " 56% 716/1269 [07:20<07:20,  1.26it/s]\u001b[A\n",
            " 57% 717/1269 [07:20<06:59,  1.32it/s]\u001b[A\n",
            " 57% 718/1269 [07:21<06:46,  1.36it/s]\u001b[A\n",
            " 57% 719/1269 [07:22<06:35,  1.39it/s]\u001b[A\n",
            " 57% 720/1269 [07:22<06:28,  1.41it/s]\u001b[A\n",
            " 57% 721/1269 [07:23<06:22,  1.43it/s]\u001b[A\n",
            " 57% 722/1269 [07:24<06:19,  1.44it/s]\u001b[A\n",
            " 57% 723/1269 [07:24<06:16,  1.45it/s]\u001b[A\n",
            " 57% 724/1269 [07:25<06:14,  1.45it/s]\u001b[A\n",
            " 57% 725/1269 [07:26<06:13,  1.45it/s]\u001b[A\n",
            " 57% 726/1269 [07:26<06:13,  1.46it/s]\u001b[A\n",
            " 57% 727/1269 [07:27<06:11,  1.46it/s]\u001b[A\n",
            " 57% 728/1269 [07:28<06:10,  1.46it/s]\u001b[A\n",
            " 57% 729/1269 [07:29<06:11,  1.46it/s]\u001b[A\n",
            " 58% 730/1269 [07:29<06:10,  1.45it/s]\u001b[A\n",
            " 58% 731/1269 [07:30<06:10,  1.45it/s]\u001b[A\n",
            " 58% 732/1269 [07:31<06:10,  1.45it/s]\u001b[A\n",
            " 58% 733/1269 [07:31<06:09,  1.45it/s]\u001b[A\n",
            " 58% 734/1269 [07:32<06:08,  1.45it/s]\u001b[A\n",
            " 58% 735/1269 [07:33<06:08,  1.45it/s]\u001b[A\n",
            " 58% 736/1269 [07:33<06:07,  1.45it/s]\u001b[A\n",
            " 58% 737/1269 [07:34<06:08,  1.44it/s]\u001b[A\n",
            " 58% 738/1269 [07:35<06:09,  1.44it/s]\u001b[A\n",
            " 58% 739/1269 [07:35<06:09,  1.44it/s]\u001b[A\n",
            " 58% 740/1269 [07:36<06:10,  1.43it/s]\u001b[A\n",
            " 58% 741/1269 [07:37<06:10,  1.42it/s]\u001b[A\n",
            " 58% 742/1269 [07:38<06:12,  1.41it/s]\u001b[A\n",
            " 59% 743/1269 [07:38<06:12,  1.41it/s]\u001b[A\n",
            " 59% 744/1269 [07:39<06:12,  1.41it/s]\u001b[A\n",
            " 59% 745/1269 [07:40<06:12,  1.41it/s]\u001b[A\n",
            " 59% 746/1269 [07:40<06:11,  1.41it/s]\u001b[A\n",
            " 59% 747/1269 [07:41<06:09,  1.41it/s]\u001b[A\n",
            " 59% 748/1269 [07:42<06:08,  1.42it/s]\u001b[A\n",
            " 59% 749/1269 [07:43<06:07,  1.41it/s]\u001b[A\n",
            " 59% 750/1269 [07:43<06:06,  1.42it/s]\u001b[A\n",
            " 59% 751/1269 [07:44<06:05,  1.42it/s]\u001b[A\n",
            " 59% 752/1269 [07:45<06:03,  1.42it/s]\u001b[A\n",
            " 59% 753/1269 [07:45<06:00,  1.43it/s]\u001b[A\n",
            " 59% 754/1269 [07:46<05:57,  1.44it/s]\u001b[A\n",
            " 59% 755/1269 [07:47<05:56,  1.44it/s]\u001b[A\n",
            " 60% 756/1269 [07:47<05:55,  1.44it/s]\u001b[A\n",
            " 60% 757/1269 [07:48<05:54,  1.45it/s]\u001b[A\n",
            " 60% 758/1269 [07:49<05:53,  1.45it/s]\u001b[A\n",
            " 60% 759/1269 [07:49<05:51,  1.45it/s]\u001b[A\n",
            " 60% 760/1269 [07:50<05:51,  1.45it/s]\u001b[A\n",
            " 60% 761/1269 [07:51<05:49,  1.45it/s]\u001b[A\n",
            " 60% 762/1269 [07:52<05:49,  1.45it/s]\u001b[A\n",
            " 60% 763/1269 [07:52<05:48,  1.45it/s]\u001b[A\n",
            " 60% 764/1269 [07:53<05:47,  1.45it/s]\u001b[A\n",
            " 60% 765/1269 [07:54<05:47,  1.45it/s]\u001b[A\n",
            " 60% 766/1269 [07:54<05:46,  1.45it/s]\u001b[A\n",
            " 60% 767/1269 [07:55<05:45,  1.45it/s]\u001b[A\n",
            " 61% 768/1269 [07:56<05:45,  1.45it/s]\u001b[A\n",
            " 61% 769/1269 [07:56<05:44,  1.45it/s]\u001b[A\n",
            " 61% 770/1269 [07:57<05:43,  1.45it/s]\u001b[A\n",
            " 61% 771/1269 [07:58<05:42,  1.45it/s]\u001b[A\n",
            " 61% 772/1269 [07:58<05:41,  1.45it/s]\u001b[A\n",
            " 61% 773/1269 [07:59<05:39,  1.46it/s]\u001b[A\n",
            " 61% 774/1269 [08:00<05:39,  1.46it/s]\u001b[A\n",
            " 61% 775/1269 [08:00<05:38,  1.46it/s]\u001b[A\n",
            " 61% 776/1269 [08:01<05:38,  1.46it/s]\u001b[A\n",
            " 61% 777/1269 [08:02<05:37,  1.46it/s]\u001b[A\n",
            " 61% 778/1269 [08:03<05:36,  1.46it/s]\u001b[A\n",
            " 61% 779/1269 [08:03<05:35,  1.46it/s]\u001b[A\n",
            " 61% 780/1269 [08:04<05:34,  1.46it/s]\u001b[A\n",
            " 62% 781/1269 [08:05<05:32,  1.47it/s]\u001b[A\n",
            " 62% 782/1269 [08:05<05:32,  1.46it/s]\u001b[A\n",
            " 62% 783/1269 [08:06<05:32,  1.46it/s]\u001b[A\n",
            " 62% 784/1269 [08:07<05:30,  1.47it/s]\u001b[A\n",
            " 62% 785/1269 [08:07<05:29,  1.47it/s]\u001b[A\n",
            " 62% 786/1269 [08:08<05:29,  1.47it/s]\u001b[A\n",
            " 62% 787/1269 [08:09<05:28,  1.47it/s]\u001b[A\n",
            " 62% 788/1269 [08:09<05:28,  1.47it/s]\u001b[A\n",
            " 62% 789/1269 [08:10<05:26,  1.47it/s]\u001b[A\n",
            " 62% 790/1269 [08:11<05:25,  1.47it/s]\u001b[A\n",
            " 62% 791/1269 [08:11<05:25,  1.47it/s]\u001b[A\n",
            " 62% 792/1269 [08:12<05:24,  1.47it/s]\u001b[A\n",
            " 62% 793/1269 [08:13<05:24,  1.47it/s]\u001b[A\n",
            " 63% 794/1269 [08:13<05:22,  1.47it/s]\u001b[A\n",
            " 63% 795/1269 [08:14<05:22,  1.47it/s]\u001b[A\n",
            " 63% 796/1269 [08:15<05:20,  1.47it/s]\u001b[A\n",
            " 63% 797/1269 [08:15<05:21,  1.47it/s]\u001b[A\n",
            " 63% 798/1269 [08:16<05:21,  1.47it/s]\u001b[A\n",
            " 63% 799/1269 [08:17<05:19,  1.47it/s]\u001b[A\n",
            " 63% 800/1269 [08:17<05:18,  1.47it/s]\u001b[A\n",
            " 63% 801/1269 [08:18<05:17,  1.47it/s]\u001b[A\n",
            " 63% 802/1269 [08:19<05:17,  1.47it/s]\u001b[A\n",
            " 63% 803/1269 [08:20<05:16,  1.47it/s]\u001b[A\n",
            " 63% 804/1269 [08:20<05:16,  1.47it/s]\u001b[A\n",
            " 63% 805/1269 [08:21<05:16,  1.47it/s]\u001b[A\n",
            " 64% 806/1269 [08:22<05:15,  1.47it/s]\u001b[A\n",
            " 64% 807/1269 [08:22<05:14,  1.47it/s]\u001b[A\n",
            " 64% 808/1269 [08:23<05:13,  1.47it/s]\u001b[A\n",
            " 64% 809/1269 [08:24<05:13,  1.47it/s]\u001b[A\n",
            " 64% 810/1269 [08:24<05:12,  1.47it/s]\u001b[A\n",
            " 64% 811/1269 [08:25<05:11,  1.47it/s]\u001b[A\n",
            " 64% 812/1269 [08:26<05:12,  1.46it/s]\u001b[A\n",
            " 64% 813/1269 [08:26<05:11,  1.46it/s]\u001b[A\n",
            " 64% 814/1269 [08:27<05:11,  1.46it/s]\u001b[A\n",
            " 64% 815/1269 [08:28<05:09,  1.46it/s]\u001b[A\n",
            " 64% 816/1269 [08:28<05:08,  1.47it/s]\u001b[A\n",
            " 64% 817/1269 [08:29<05:07,  1.47it/s]\u001b[A\n",
            " 64% 818/1269 [08:30<05:07,  1.47it/s]\u001b[A\n",
            " 65% 819/1269 [08:30<05:07,  1.46it/s]\u001b[A\n",
            " 65% 820/1269 [08:31<05:06,  1.47it/s]\u001b[A\n",
            " 65% 821/1269 [08:32<05:05,  1.47it/s]\u001b[A\n",
            " 65% 822/1269 [08:32<05:05,  1.47it/s]\u001b[A\n",
            " 65% 823/1269 [08:33<05:05,  1.46it/s]\u001b[A\n",
            " 65% 824/1269 [08:34<05:04,  1.46it/s]\u001b[A\n",
            " 65% 825/1269 [08:35<05:03,  1.46it/s]\u001b[A\n",
            " 65% 826/1269 [08:35<05:02,  1.46it/s]\u001b[A\n",
            " 65% 827/1269 [08:36<05:02,  1.46it/s]\u001b[A\n",
            " 65% 828/1269 [08:37<05:00,  1.47it/s]\u001b[A\n",
            " 65% 829/1269 [08:37<05:00,  1.47it/s]\u001b[A\n",
            " 65% 830/1269 [08:38<04:59,  1.47it/s]\u001b[A\n",
            " 65% 831/1269 [08:39<04:59,  1.46it/s]\u001b[A\n",
            " 66% 832/1269 [08:39<04:58,  1.47it/s]\u001b[A\n",
            " 66% 833/1269 [08:40<04:58,  1.46it/s]\u001b[A\n",
            " 66% 834/1269 [08:41<04:57,  1.46it/s]\u001b[A\n",
            " 66% 835/1269 [08:41<04:56,  1.46it/s]\u001b[A\n",
            " 66% 836/1269 [08:42<04:56,  1.46it/s]\u001b[A\n",
            " 66% 837/1269 [08:43<04:55,  1.46it/s]\u001b[A\n",
            " 66% 838/1269 [08:43<04:54,  1.46it/s]\u001b[A\n",
            " 66% 839/1269 [08:44<04:54,  1.46it/s]\u001b[A\n",
            " 66% 840/1269 [08:45<04:53,  1.46it/s]\u001b[A\n",
            " 66% 841/1269 [08:45<04:53,  1.46it/s]\u001b[A\n",
            " 66% 842/1269 [08:46<04:52,  1.46it/s]\u001b[A\n",
            " 66% 843/1269 [08:47<04:50,  1.47it/s]\u001b[A\n",
            " 67% 844/1269 [08:48<04:50,  1.46it/s]\u001b[A\n",
            " 67% 845/1269 [08:48<04:49,  1.47it/s]\u001b[A\n",
            " 67% 846/1269 [08:48<03:59,  1.77it/s]\u001b[A[INFO|trainer.py:726] 2022-11-04 16:50:26,311 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:50:26,313 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:50:26,313 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:50:26,313 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.903475284576416, 'eval_accuracy': 0.7293639779090881, 'eval_f1': 0.7292207126002139, 'eval_runtime': 5.6077, 'eval_samples_per_second': 131.783, 'eval_steps_per_second': 16.584, 'epoch': 6.0}\n",
            "100% 93/93 [00:05<00:00, 16.59it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:50:31,922 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-846\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:50:31,923 >> Configuration saved in models/ZeroShot/0/checkpoint-846/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:50:34,515 >> Model weights saved in models/ZeroShot/0/checkpoint-846/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:50:34,516 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-846/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:50:34,516 >> Special tokens file saved in models/ZeroShot/0/checkpoint-846/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 16:50:40,432 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-705] due to args.save_total_limit\n",
            "\n",
            " 67% 847/1269 [09:04<34:27,  4.90s/it]\u001b[A\n",
            " 67% 848/1269 [09:04<25:27,  3.63s/it]\u001b[A\n",
            " 67% 849/1269 [09:05<19:12,  2.74s/it]\u001b[A\n",
            " 67% 850/1269 [09:06<14:49,  2.12s/it]\u001b[A\n",
            " 67% 851/1269 [09:06<11:46,  1.69s/it]\u001b[A\n",
            " 67% 852/1269 [09:07<09:37,  1.39s/it]\u001b[A\n",
            " 67% 853/1269 [09:08<08:07,  1.17s/it]\u001b[A\n",
            " 67% 854/1269 [09:08<07:06,  1.03s/it]\u001b[A\n",
            " 67% 855/1269 [09:09<06:21,  1.09it/s]\u001b[A\n",
            " 67% 856/1269 [09:10<05:51,  1.18it/s]\u001b[A\n",
            " 68% 857/1269 [09:10<05:30,  1.25it/s]\u001b[A\n",
            " 68% 858/1269 [09:11<05:14,  1.31it/s]\u001b[A\n",
            " 68% 859/1269 [09:12<05:03,  1.35it/s]\u001b[A\n",
            " 68% 860/1269 [09:12<04:55,  1.38it/s]\u001b[A\n",
            " 68% 861/1269 [09:13<04:50,  1.40it/s]\u001b[A\n",
            " 68% 862/1269 [09:14<04:46,  1.42it/s]\u001b[A\n",
            " 68% 863/1269 [09:14<04:42,  1.44it/s]\u001b[A\n",
            " 68% 864/1269 [09:15<04:41,  1.44it/s]\u001b[A\n",
            " 68% 865/1269 [09:16<04:39,  1.44it/s]\u001b[A\n",
            " 68% 866/1269 [09:16<04:38,  1.45it/s]\u001b[A\n",
            " 68% 867/1269 [09:17<04:37,  1.45it/s]\u001b[A\n",
            " 68% 868/1269 [09:18<04:36,  1.45it/s]\u001b[A\n",
            " 68% 869/1269 [09:19<04:36,  1.45it/s]\u001b[A\n",
            " 69% 870/1269 [09:19<04:35,  1.45it/s]\u001b[A\n",
            " 69% 871/1269 [09:20<04:34,  1.45it/s]\u001b[A\n",
            " 69% 872/1269 [09:21<04:33,  1.45it/s]\u001b[A\n",
            " 69% 873/1269 [09:21<04:33,  1.45it/s]\u001b[A\n",
            " 69% 874/1269 [09:22<04:32,  1.45it/s]\u001b[A\n",
            " 69% 875/1269 [09:23<04:31,  1.45it/s]\u001b[A\n",
            " 69% 876/1269 [09:23<04:30,  1.45it/s]\u001b[A\n",
            " 69% 877/1269 [09:24<04:30,  1.45it/s]\u001b[A\n",
            " 69% 878/1269 [09:25<04:32,  1.44it/s]\u001b[A\n",
            " 69% 879/1269 [09:25<04:31,  1.44it/s]\u001b[A\n",
            " 69% 880/1269 [09:26<04:29,  1.44it/s]\u001b[A\n",
            " 69% 881/1269 [09:27<04:29,  1.44it/s]\u001b[A\n",
            " 70% 882/1269 [09:28<04:28,  1.44it/s]\u001b[A\n",
            " 70% 883/1269 [09:28<04:28,  1.44it/s]\u001b[A\n",
            " 70% 884/1269 [09:29<04:26,  1.44it/s]\u001b[A\n",
            " 70% 885/1269 [09:30<04:26,  1.44it/s]\u001b[A\n",
            " 70% 886/1269 [09:30<04:24,  1.45it/s]\u001b[A\n",
            " 70% 887/1269 [09:31<04:23,  1.45it/s]\u001b[A\n",
            " 70% 888/1269 [09:32<04:22,  1.45it/s]\u001b[A\n",
            " 70% 889/1269 [09:32<04:21,  1.45it/s]\u001b[A\n",
            " 70% 890/1269 [09:33<04:21,  1.45it/s]\u001b[A\n",
            " 70% 891/1269 [09:34<04:19,  1.45it/s]\u001b[A\n",
            " 70% 892/1269 [09:34<04:19,  1.45it/s]\u001b[A\n",
            " 70% 893/1269 [09:35<04:19,  1.45it/s]\u001b[A\n",
            " 70% 894/1269 [09:36<04:18,  1.45it/s]\u001b[A\n",
            " 71% 895/1269 [09:36<04:17,  1.45it/s]\u001b[A\n",
            " 71% 896/1269 [09:37<04:16,  1.45it/s]\u001b[A\n",
            " 71% 897/1269 [09:38<04:16,  1.45it/s]\u001b[A\n",
            " 71% 898/1269 [09:39<04:15,  1.45it/s]\u001b[A\n",
            " 71% 899/1269 [09:39<04:14,  1.45it/s]\u001b[A\n",
            " 71% 900/1269 [09:40<04:14,  1.45it/s]\u001b[A\n",
            " 71% 901/1269 [09:41<04:13,  1.45it/s]\u001b[A\n",
            " 71% 902/1269 [09:41<04:12,  1.45it/s]\u001b[A\n",
            " 71% 903/1269 [09:42<04:12,  1.45it/s]\u001b[A\n",
            " 71% 904/1269 [09:43<04:11,  1.45it/s]\u001b[A\n",
            " 71% 905/1269 [09:43<04:10,  1.45it/s]\u001b[A\n",
            " 71% 906/1269 [09:44<04:10,  1.45it/s]\u001b[A\n",
            " 71% 907/1269 [09:45<04:09,  1.45it/s]\u001b[A\n",
            " 72% 908/1269 [09:45<04:08,  1.45it/s]\u001b[A\n",
            " 72% 909/1269 [09:46<04:08,  1.45it/s]\u001b[A\n",
            " 72% 910/1269 [09:47<04:08,  1.45it/s]\u001b[A\n",
            " 72% 911/1269 [09:48<04:06,  1.45it/s]\u001b[A\n",
            " 72% 912/1269 [09:48<04:06,  1.45it/s]\u001b[A\n",
            " 72% 913/1269 [09:49<04:05,  1.45it/s]\u001b[A\n",
            " 72% 914/1269 [09:50<04:05,  1.45it/s]\u001b[A\n",
            " 72% 915/1269 [09:50<04:04,  1.45it/s]\u001b[A\n",
            " 72% 916/1269 [09:51<04:03,  1.45it/s]\u001b[A\n",
            " 72% 917/1269 [09:52<04:02,  1.45it/s]\u001b[A\n",
            " 72% 918/1269 [09:52<04:02,  1.45it/s]\u001b[A\n",
            " 72% 919/1269 [09:53<04:01,  1.45it/s]\u001b[A\n",
            " 72% 920/1269 [09:54<04:00,  1.45it/s]\u001b[A\n",
            " 73% 921/1269 [09:54<03:58,  1.46it/s]\u001b[A\n",
            " 73% 922/1269 [09:55<03:57,  1.46it/s]\u001b[A\n",
            " 73% 923/1269 [09:56<03:56,  1.46it/s]\u001b[A\n",
            " 73% 924/1269 [09:56<03:55,  1.46it/s]\u001b[A\n",
            " 73% 925/1269 [09:57<03:55,  1.46it/s]\u001b[A\n",
            " 73% 926/1269 [09:58<03:55,  1.46it/s]\u001b[A\n",
            " 73% 927/1269 [09:59<03:54,  1.46it/s]\u001b[A\n",
            " 73% 928/1269 [09:59<03:54,  1.46it/s]\u001b[A\n",
            " 73% 929/1269 [10:00<03:53,  1.46it/s]\u001b[A\n",
            " 73% 930/1269 [10:01<03:51,  1.46it/s]\u001b[A\n",
            " 73% 931/1269 [10:01<03:50,  1.46it/s]\u001b[A\n",
            " 73% 932/1269 [10:02<03:50,  1.46it/s]\u001b[A\n",
            " 74% 933/1269 [10:03<03:49,  1.47it/s]\u001b[A\n",
            " 74% 934/1269 [10:03<03:48,  1.46it/s]\u001b[A\n",
            " 74% 935/1269 [10:04<03:48,  1.46it/s]\u001b[A\n",
            " 74% 936/1269 [10:05<03:46,  1.47it/s]\u001b[A\n",
            " 74% 937/1269 [10:05<03:46,  1.47it/s]\u001b[A\n",
            " 74% 938/1269 [10:06<03:45,  1.47it/s]\u001b[A\n",
            " 74% 939/1269 [10:07<03:45,  1.47it/s]\u001b[A\n",
            " 74% 940/1269 [10:07<03:44,  1.47it/s]\u001b[A\n",
            " 74% 941/1269 [10:08<03:43,  1.47it/s]\u001b[A\n",
            " 74% 942/1269 [10:09<03:42,  1.47it/s]\u001b[A\n",
            " 74% 943/1269 [10:09<03:41,  1.47it/s]\u001b[A\n",
            " 74% 944/1269 [10:10<03:41,  1.47it/s]\u001b[A\n",
            " 74% 945/1269 [10:11<03:40,  1.47it/s]\u001b[A\n",
            " 75% 946/1269 [10:11<03:39,  1.47it/s]\u001b[A\n",
            " 75% 947/1269 [10:12<03:38,  1.47it/s]\u001b[A\n",
            " 75% 948/1269 [10:13<03:38,  1.47it/s]\u001b[A\n",
            " 75% 949/1269 [10:13<03:37,  1.47it/s]\u001b[A\n",
            " 75% 950/1269 [10:14<03:36,  1.47it/s]\u001b[A\n",
            " 75% 951/1269 [10:15<03:36,  1.47it/s]\u001b[A\n",
            " 75% 952/1269 [10:16<03:35,  1.47it/s]\u001b[A\n",
            " 75% 953/1269 [10:16<03:35,  1.47it/s]\u001b[A\n",
            " 75% 954/1269 [10:17<03:33,  1.47it/s]\u001b[A\n",
            " 75% 955/1269 [10:18<03:33,  1.47it/s]\u001b[A\n",
            " 75% 956/1269 [10:18<03:33,  1.47it/s]\u001b[A\n",
            " 75% 957/1269 [10:19<03:32,  1.47it/s]\u001b[A\n",
            " 75% 958/1269 [10:20<03:32,  1.46it/s]\u001b[A\n",
            " 76% 959/1269 [10:20<03:31,  1.46it/s]\u001b[A\n",
            " 76% 960/1269 [10:21<03:31,  1.46it/s]\u001b[A\n",
            " 76% 961/1269 [10:22<03:30,  1.46it/s]\u001b[A\n",
            " 76% 962/1269 [10:22<03:29,  1.47it/s]\u001b[A\n",
            " 76% 963/1269 [10:23<03:28,  1.47it/s]\u001b[A\n",
            " 76% 964/1269 [10:24<03:27,  1.47it/s]\u001b[A\n",
            " 76% 965/1269 [10:24<03:27,  1.47it/s]\u001b[A\n",
            " 76% 966/1269 [10:25<03:25,  1.47it/s]\u001b[A\n",
            " 76% 967/1269 [10:26<03:24,  1.47it/s]\u001b[A\n",
            " 76% 968/1269 [10:26<03:24,  1.47it/s]\u001b[A\n",
            " 76% 969/1269 [10:27<03:24,  1.47it/s]\u001b[A\n",
            " 76% 970/1269 [10:28<03:23,  1.47it/s]\u001b[A\n",
            " 77% 971/1269 [10:28<03:22,  1.47it/s]\u001b[A\n",
            " 77% 972/1269 [10:29<03:22,  1.47it/s]\u001b[A\n",
            " 77% 973/1269 [10:30<03:21,  1.47it/s]\u001b[A\n",
            " 77% 974/1269 [10:31<03:20,  1.47it/s]\u001b[A\n",
            " 77% 975/1269 [10:31<03:20,  1.47it/s]\u001b[A\n",
            " 77% 976/1269 [10:32<03:19,  1.47it/s]\u001b[A\n",
            " 77% 977/1269 [10:33<03:19,  1.47it/s]\u001b[A\n",
            " 77% 978/1269 [10:33<03:18,  1.47it/s]\u001b[A\n",
            " 77% 979/1269 [10:34<03:17,  1.47it/s]\u001b[A\n",
            " 77% 980/1269 [10:35<03:17,  1.47it/s]\u001b[A\n",
            " 77% 981/1269 [10:35<03:16,  1.46it/s]\u001b[A\n",
            " 77% 982/1269 [10:36<03:15,  1.47it/s]\u001b[A\n",
            " 77% 983/1269 [10:37<03:15,  1.46it/s]\u001b[A\n",
            " 78% 984/1269 [10:37<03:14,  1.47it/s]\u001b[A\n",
            " 78% 985/1269 [10:38<03:13,  1.47it/s]\u001b[A\n",
            " 78% 986/1269 [10:39<03:12,  1.47it/s]\u001b[A\n",
            " 78% 987/1269 [10:39<02:39,  1.77it/s]\u001b[A[INFO|trainer.py:726] 2022-11-04 16:52:16,810 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:52:16,812 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:52:16,812 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:52:16,812 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 2.0325510501861572, 'eval_accuracy': 0.7280108332633972, 'eval_f1': 0.7278114640997873, 'eval_runtime': 5.62, 'eval_samples_per_second': 131.495, 'eval_steps_per_second': 16.548, 'epoch': 7.0}\n",
            "100% 93/93 [00:05<00:00, 16.62it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:52:22,433 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-987\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:52:22,434 >> Configuration saved in models/ZeroShot/0/checkpoint-987/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:52:25,031 >> Model weights saved in models/ZeroShot/0/checkpoint-987/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:52:25,032 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-987/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:52:25,032 >> Special tokens file saved in models/ZeroShot/0/checkpoint-987/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 16:52:30,832 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-846] due to args.save_total_limit\n",
            "\n",
            " 78% 988/1269 [10:54<22:46,  4.86s/it]\u001b[A\n",
            " 78% 989/1269 [10:55<16:49,  3.60s/it]\u001b[A\n",
            " 78% 990/1269 [10:55<12:40,  2.73s/it]\u001b[A\n",
            " 78% 991/1269 [10:56<09:46,  2.11s/it]\u001b[A\n",
            " 78% 992/1269 [10:57<07:45,  1.68s/it]\u001b[A\n",
            " 78% 993/1269 [10:57<06:20,  1.38s/it]\u001b[A\n",
            " 78% 994/1269 [10:58<05:21,  1.17s/it]\u001b[A\n",
            " 78% 995/1269 [10:59<04:40,  1.02s/it]\u001b[A\n",
            " 78% 996/1269 [10:59<04:10,  1.09it/s]\u001b[A\n",
            " 79% 997/1269 [11:00<03:50,  1.18it/s]\u001b[A\n",
            " 79% 998/1269 [11:01<03:35,  1.26it/s]\u001b[A\n",
            " 79% 999/1269 [11:01<03:25,  1.32it/s]\u001b[A\n",
            " 79% 1000/1269 [11:02<03:18,  1.36it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0219, 'learning_rate': 4.239558707643815e-06, 'epoch': 7.09}\n",
            "\n",
            " 79% 1000/1269 [11:02<03:18,  1.36it/s]\u001b[A\n",
            " 79% 1001/1269 [11:03<03:13,  1.38it/s]\u001b[A\n",
            " 79% 1002/1269 [11:03<03:10,  1.40it/s]\u001b[A\n",
            " 79% 1003/1269 [11:04<03:07,  1.42it/s]\u001b[A\n",
            " 79% 1004/1269 [11:05<03:05,  1.43it/s]\u001b[A\n",
            " 79% 1005/1269 [11:05<03:03,  1.44it/s]\u001b[A\n",
            " 79% 1006/1269 [11:06<03:02,  1.44it/s]\u001b[A\n",
            " 79% 1007/1269 [11:07<03:01,  1.45it/s]\u001b[A\n",
            " 79% 1008/1269 [11:08<02:59,  1.45it/s]\u001b[A\n",
            " 80% 1009/1269 [11:08<02:59,  1.45it/s]\u001b[A\n",
            " 80% 1010/1269 [11:09<02:58,  1.45it/s]\u001b[A\n",
            " 80% 1011/1269 [11:10<02:57,  1.45it/s]\u001b[A\n",
            " 80% 1012/1269 [11:10<02:57,  1.45it/s]\u001b[A\n",
            " 80% 1013/1269 [11:11<02:56,  1.45it/s]\u001b[A\n",
            " 80% 1014/1269 [11:12<02:55,  1.45it/s]\u001b[A\n",
            " 80% 1015/1269 [11:12<02:55,  1.45it/s]\u001b[A\n",
            " 80% 1016/1269 [11:13<02:54,  1.45it/s]\u001b[A\n",
            " 80% 1017/1269 [11:14<02:53,  1.45it/s]\u001b[A\n",
            " 80% 1018/1269 [11:14<02:53,  1.45it/s]\u001b[A\n",
            " 80% 1019/1269 [11:15<02:52,  1.45it/s]\u001b[A\n",
            " 80% 1020/1269 [11:16<02:51,  1.45it/s]\u001b[A\n",
            " 80% 1021/1269 [11:16<02:50,  1.45it/s]\u001b[A\n",
            " 81% 1022/1269 [11:17<02:50,  1.45it/s]\u001b[A\n",
            " 81% 1023/1269 [11:18<02:49,  1.45it/s]\u001b[A\n",
            " 81% 1024/1269 [11:19<02:48,  1.45it/s]\u001b[A\n",
            " 81% 1025/1269 [11:19<02:48,  1.45it/s]\u001b[A\n",
            " 81% 1026/1269 [11:20<02:47,  1.45it/s]\u001b[A\n",
            " 81% 1027/1269 [11:21<02:47,  1.44it/s]\u001b[A\n",
            " 81% 1028/1269 [11:21<02:46,  1.44it/s]\u001b[A\n",
            " 81% 1029/1269 [11:22<02:46,  1.45it/s]\u001b[A\n",
            " 81% 1030/1269 [11:23<02:45,  1.45it/s]\u001b[A\n",
            " 81% 1031/1269 [11:23<02:44,  1.45it/s]\u001b[A\n",
            " 81% 1032/1269 [11:24<02:43,  1.45it/s]\u001b[A\n",
            " 81% 1033/1269 [11:25<02:42,  1.45it/s]\u001b[A\n",
            " 81% 1034/1269 [11:25<02:41,  1.45it/s]\u001b[A\n",
            " 82% 1035/1269 [11:26<02:41,  1.45it/s]\u001b[A\n",
            " 82% 1036/1269 [11:27<02:40,  1.45it/s]\u001b[A\n",
            " 82% 1037/1269 [11:28<02:39,  1.45it/s]\u001b[A\n",
            " 82% 1038/1269 [11:28<02:38,  1.45it/s]\u001b[A\n",
            " 82% 1039/1269 [11:29<02:38,  1.45it/s]\u001b[A\n",
            " 82% 1040/1269 [11:30<02:37,  1.45it/s]\u001b[A\n",
            " 82% 1041/1269 [11:30<02:37,  1.45it/s]\u001b[A\n",
            " 82% 1042/1269 [11:31<02:36,  1.45it/s]\u001b[A\n",
            " 82% 1043/1269 [11:32<02:35,  1.45it/s]\u001b[A\n",
            " 82% 1044/1269 [11:32<02:35,  1.45it/s]\u001b[A\n",
            " 82% 1045/1269 [11:33<02:34,  1.45it/s]\u001b[A\n",
            " 82% 1046/1269 [11:34<02:33,  1.45it/s]\u001b[A\n",
            " 83% 1047/1269 [11:34<02:32,  1.45it/s]\u001b[A\n",
            " 83% 1048/1269 [11:35<02:32,  1.45it/s]\u001b[A\n",
            " 83% 1049/1269 [11:36<02:31,  1.45it/s]\u001b[A\n",
            " 83% 1050/1269 [11:36<02:30,  1.45it/s]\u001b[A\n",
            " 83% 1051/1269 [11:37<02:30,  1.45it/s]\u001b[A\n",
            " 83% 1052/1269 [11:38<02:29,  1.45it/s]\u001b[A\n",
            " 83% 1053/1269 [11:39<02:28,  1.45it/s]\u001b[A\n",
            " 83% 1054/1269 [11:39<02:28,  1.45it/s]\u001b[A\n",
            " 83% 1055/1269 [11:40<02:27,  1.45it/s]\u001b[A\n",
            " 83% 1056/1269 [11:41<02:26,  1.45it/s]\u001b[A\n",
            " 83% 1057/1269 [11:41<02:25,  1.45it/s]\u001b[A\n",
            " 83% 1058/1269 [11:42<02:25,  1.45it/s]\u001b[A\n",
            " 83% 1059/1269 [11:43<02:23,  1.46it/s]\u001b[A\n",
            " 84% 1060/1269 [11:43<02:23,  1.46it/s]\u001b[A\n",
            " 84% 1061/1269 [11:44<02:22,  1.46it/s]\u001b[A\n",
            " 84% 1062/1269 [11:45<02:21,  1.46it/s]\u001b[A\n",
            " 84% 1063/1269 [11:45<02:20,  1.46it/s]\u001b[A\n",
            " 84% 1064/1269 [11:46<02:19,  1.46it/s]\u001b[A\n",
            " 84% 1065/1269 [11:47<02:18,  1.47it/s]\u001b[A\n",
            " 84% 1066/1269 [11:47<02:17,  1.47it/s]\u001b[A\n",
            " 84% 1067/1269 [11:48<02:17,  1.47it/s]\u001b[A\n",
            " 84% 1068/1269 [11:49<02:16,  1.47it/s]\u001b[A\n",
            " 84% 1069/1269 [11:49<02:16,  1.47it/s]\u001b[A\n",
            " 84% 1070/1269 [11:50<02:15,  1.47it/s]\u001b[A\n",
            " 84% 1071/1269 [11:51<02:14,  1.47it/s]\u001b[A\n",
            " 84% 1072/1269 [11:52<02:13,  1.47it/s]\u001b[A\n",
            " 85% 1073/1269 [11:52<02:13,  1.47it/s]\u001b[A\n",
            " 85% 1074/1269 [11:53<02:12,  1.47it/s]\u001b[A\n",
            " 85% 1075/1269 [11:54<02:11,  1.47it/s]\u001b[A\n",
            " 85% 1076/1269 [11:54<02:11,  1.47it/s]\u001b[A\n",
            " 85% 1077/1269 [11:55<02:10,  1.47it/s]\u001b[A\n",
            " 85% 1078/1269 [11:56<02:09,  1.47it/s]\u001b[A\n",
            " 85% 1079/1269 [11:56<02:08,  1.47it/s]\u001b[A\n",
            " 85% 1080/1269 [11:57<02:08,  1.47it/s]\u001b[A\n",
            " 85% 1081/1269 [11:58<02:07,  1.47it/s]\u001b[A\n",
            " 85% 1082/1269 [11:58<02:06,  1.47it/s]\u001b[A\n",
            " 85% 1083/1269 [11:59<02:06,  1.47it/s]\u001b[A\n",
            " 85% 1084/1269 [12:00<02:05,  1.47it/s]\u001b[A\n",
            " 86% 1085/1269 [12:00<02:04,  1.47it/s]\u001b[A\n",
            " 86% 1086/1269 [12:01<02:04,  1.48it/s]\u001b[A\n",
            " 86% 1087/1269 [12:02<02:03,  1.47it/s]\u001b[A\n",
            " 86% 1088/1269 [12:02<02:02,  1.47it/s]\u001b[A\n",
            " 86% 1089/1269 [12:03<02:02,  1.47it/s]\u001b[A\n",
            " 86% 1090/1269 [12:04<02:01,  1.47it/s]\u001b[A\n",
            " 86% 1091/1269 [12:04<02:00,  1.47it/s]\u001b[A\n",
            " 86% 1092/1269 [12:05<02:00,  1.47it/s]\u001b[A\n",
            " 86% 1093/1269 [12:06<01:59,  1.47it/s]\u001b[A\n",
            " 86% 1094/1269 [12:06<01:58,  1.47it/s]\u001b[A\n",
            " 86% 1095/1269 [12:07<01:58,  1.47it/s]\u001b[A\n",
            " 86% 1096/1269 [12:08<01:57,  1.47it/s]\u001b[A\n",
            " 86% 1097/1269 [12:08<01:56,  1.47it/s]\u001b[A\n",
            " 87% 1098/1269 [12:09<01:56,  1.47it/s]\u001b[A\n",
            " 87% 1099/1269 [12:10<01:55,  1.47it/s]\u001b[A\n",
            " 87% 1100/1269 [12:11<01:54,  1.47it/s]\u001b[A\n",
            " 87% 1101/1269 [12:11<01:54,  1.47it/s]\u001b[A\n",
            " 87% 1102/1269 [12:12<01:53,  1.47it/s]\u001b[A\n",
            " 87% 1103/1269 [12:13<01:52,  1.47it/s]\u001b[A\n",
            " 87% 1104/1269 [12:13<01:52,  1.47it/s]\u001b[A\n",
            " 87% 1105/1269 [12:14<01:51,  1.47it/s]\u001b[A\n",
            " 87% 1106/1269 [12:15<01:50,  1.47it/s]\u001b[A\n",
            " 87% 1107/1269 [12:15<01:50,  1.47it/s]\u001b[A\n",
            " 87% 1108/1269 [12:16<01:49,  1.47it/s]\u001b[A\n",
            " 87% 1109/1269 [12:17<01:48,  1.47it/s]\u001b[A\n",
            " 87% 1110/1269 [12:17<01:48,  1.47it/s]\u001b[A\n",
            " 88% 1111/1269 [12:18<01:47,  1.47it/s]\u001b[A\n",
            " 88% 1112/1269 [12:19<01:47,  1.47it/s]\u001b[A\n",
            " 88% 1113/1269 [12:19<01:46,  1.47it/s]\u001b[A\n",
            " 88% 1114/1269 [12:20<01:45,  1.47it/s]\u001b[A\n",
            " 88% 1115/1269 [12:21<01:44,  1.47it/s]\u001b[A\n",
            " 88% 1116/1269 [12:21<01:43,  1.47it/s]\u001b[A\n",
            " 88% 1117/1269 [12:22<01:43,  1.47it/s]\u001b[A\n",
            " 88% 1118/1269 [12:23<01:42,  1.47it/s]\u001b[A\n",
            " 88% 1119/1269 [12:23<01:42,  1.47it/s]\u001b[A\n",
            " 88% 1120/1269 [12:24<01:41,  1.47it/s]\u001b[A\n",
            " 88% 1121/1269 [12:25<01:40,  1.47it/s]\u001b[A\n",
            " 88% 1122/1269 [12:25<01:39,  1.47it/s]\u001b[A\n",
            " 88% 1123/1269 [12:26<01:39,  1.47it/s]\u001b[A\n",
            " 89% 1124/1269 [12:27<01:39,  1.46it/s]\u001b[A\n",
            " 89% 1125/1269 [12:28<01:38,  1.46it/s]\u001b[A\n",
            " 89% 1126/1269 [12:28<01:37,  1.46it/s]\u001b[A\n",
            " 89% 1127/1269 [12:29<01:36,  1.47it/s]\u001b[A\n",
            " 89% 1128/1269 [12:29<01:19,  1.77it/s]\u001b[A[INFO|trainer.py:726] 2022-11-04 16:54:07,013 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:54:07,015 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:54:07,015 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:54:07,015 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 2.106722593307495, 'eval_accuracy': 0.7293639779090881, 'eval_f1': 0.7292802297637888, 'eval_runtime': 5.6131, 'eval_samples_per_second': 131.656, 'eval_steps_per_second': 16.568, 'epoch': 8.0}\n",
            "100% 93/93 [00:05<00:00, 16.59it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:54:12,629 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1128\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:54:12,630 >> Configuration saved in models/ZeroShot/0/checkpoint-1128/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:54:15,223 >> Model weights saved in models/ZeroShot/0/checkpoint-1128/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:54:15,223 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1128/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:54:15,224 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1128/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 16:54:21,131 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-987] due to args.save_total_limit\n",
            "\n",
            " 89% 1129/1269 [12:44<11:27,  4.91s/it]\u001b[A\n",
            " 89% 1130/1269 [12:45<08:25,  3.64s/it]\u001b[A\n",
            " 89% 1131/1269 [12:46<06:20,  2.76s/it]\u001b[A\n",
            " 89% 1132/1269 [12:46<04:52,  2.14s/it]\u001b[A\n",
            " 89% 1133/1269 [12:47<03:51,  1.71s/it]\u001b[A\n",
            " 89% 1134/1269 [12:48<03:08,  1.40s/it]\u001b[A\n",
            " 89% 1135/1269 [12:48<02:39,  1.19s/it]\u001b[A\n",
            " 90% 1136/1269 [12:49<02:18,  1.04s/it]\u001b[A\n",
            " 90% 1137/1269 [12:50<02:03,  1.07it/s]\u001b[A\n",
            " 90% 1138/1269 [12:50<01:53,  1.15it/s]\u001b[A\n",
            " 90% 1139/1269 [12:51<01:45,  1.23it/s]\u001b[A\n",
            " 90% 1140/1269 [12:52<01:40,  1.29it/s]\u001b[A\n",
            " 90% 1141/1269 [12:53<01:36,  1.33it/s]\u001b[A\n",
            " 90% 1142/1269 [12:53<01:33,  1.36it/s]\u001b[A\n",
            " 90% 1143/1269 [12:54<01:30,  1.39it/s]\u001b[A\n",
            " 90% 1144/1269 [12:55<01:28,  1.41it/s]\u001b[A\n",
            " 90% 1145/1269 [12:55<01:27,  1.42it/s]\u001b[A\n",
            " 90% 1146/1269 [12:56<01:25,  1.43it/s]\u001b[A\n",
            " 90% 1147/1269 [12:57<01:24,  1.44it/s]\u001b[A\n",
            " 90% 1148/1269 [12:57<01:24,  1.44it/s]\u001b[A\n",
            " 91% 1149/1269 [12:58<01:23,  1.44it/s]\u001b[A\n",
            " 91% 1150/1269 [12:59<01:22,  1.45it/s]\u001b[A\n",
            " 91% 1151/1269 [12:59<01:21,  1.45it/s]\u001b[A\n",
            " 91% 1152/1269 [13:00<01:20,  1.45it/s]\u001b[A\n",
            " 91% 1153/1269 [13:01<01:20,  1.45it/s]\u001b[A\n",
            " 91% 1154/1269 [13:02<01:19,  1.45it/s]\u001b[A\n",
            " 91% 1155/1269 [13:02<01:18,  1.45it/s]\u001b[A\n",
            " 91% 1156/1269 [13:03<01:17,  1.45it/s]\u001b[A\n",
            " 91% 1157/1269 [13:04<01:17,  1.45it/s]\u001b[A\n",
            " 91% 1158/1269 [13:04<01:16,  1.45it/s]\u001b[A\n",
            " 91% 1159/1269 [13:05<01:15,  1.45it/s]\u001b[A\n",
            " 91% 1160/1269 [13:06<01:15,  1.45it/s]\u001b[A\n",
            " 91% 1161/1269 [13:06<01:14,  1.45it/s]\u001b[A\n",
            " 92% 1162/1269 [13:07<01:13,  1.45it/s]\u001b[A\n",
            " 92% 1163/1269 [13:08<01:13,  1.45it/s]\u001b[A\n",
            " 92% 1164/1269 [13:08<01:12,  1.45it/s]\u001b[A\n",
            " 92% 1165/1269 [13:09<01:11,  1.44it/s]\u001b[A\n",
            " 92% 1166/1269 [13:10<01:11,  1.45it/s]\u001b[A\n",
            " 92% 1167/1269 [13:11<01:10,  1.44it/s]\u001b[A\n",
            " 92% 1168/1269 [13:11<01:10,  1.44it/s]\u001b[A\n",
            " 92% 1169/1269 [13:12<01:09,  1.44it/s]\u001b[A\n",
            " 92% 1170/1269 [13:13<01:08,  1.44it/s]\u001b[A\n",
            " 92% 1171/1269 [13:13<01:07,  1.45it/s]\u001b[A\n",
            " 92% 1172/1269 [13:14<01:07,  1.44it/s]\u001b[A\n",
            " 92% 1173/1269 [13:15<01:06,  1.45it/s]\u001b[A\n",
            " 93% 1174/1269 [13:15<01:05,  1.45it/s]\u001b[A\n",
            " 93% 1175/1269 [13:16<01:04,  1.45it/s]\u001b[A\n",
            " 93% 1176/1269 [13:17<01:04,  1.45it/s]\u001b[A\n",
            " 93% 1177/1269 [13:17<01:03,  1.45it/s]\u001b[A\n",
            " 93% 1178/1269 [13:18<01:02,  1.45it/s]\u001b[A\n",
            " 93% 1179/1269 [13:19<01:02,  1.45it/s]\u001b[A\n",
            " 93% 1180/1269 [13:19<01:01,  1.45it/s]\u001b[A\n",
            " 93% 1181/1269 [13:20<01:00,  1.45it/s]\u001b[A\n",
            " 93% 1182/1269 [13:21<00:59,  1.45it/s]\u001b[A\n",
            " 93% 1183/1269 [13:22<00:59,  1.45it/s]\u001b[A\n",
            " 93% 1184/1269 [13:22<00:58,  1.45it/s]\u001b[A\n",
            " 93% 1185/1269 [13:23<00:57,  1.45it/s]\u001b[A\n",
            " 93% 1186/1269 [13:24<00:57,  1.45it/s]\u001b[A\n",
            " 94% 1187/1269 [13:24<00:56,  1.45it/s]\u001b[A\n",
            " 94% 1188/1269 [13:25<00:55,  1.45it/s]\u001b[A\n",
            " 94% 1189/1269 [13:26<00:55,  1.45it/s]\u001b[A\n",
            " 94% 1190/1269 [13:26<00:54,  1.45it/s]\u001b[A\n",
            " 94% 1191/1269 [13:27<00:53,  1.45it/s]\u001b[A\n",
            " 94% 1192/1269 [13:28<00:53,  1.45it/s]\u001b[A\n",
            " 94% 1193/1269 [13:28<00:52,  1.45it/s]\u001b[A\n",
            " 94% 1194/1269 [13:29<00:51,  1.45it/s]\u001b[A\n",
            " 94% 1195/1269 [13:30<00:50,  1.45it/s]\u001b[A\n",
            " 94% 1196/1269 [13:31<00:50,  1.45it/s]\u001b[A\n",
            " 94% 1197/1269 [13:31<00:49,  1.45it/s]\u001b[A\n",
            " 94% 1198/1269 [13:32<00:49,  1.44it/s]\u001b[A\n",
            " 94% 1199/1269 [13:33<00:48,  1.44it/s]\u001b[A\n",
            " 95% 1200/1269 [13:33<00:47,  1.44it/s]\u001b[A\n",
            " 95% 1201/1269 [13:34<00:47,  1.43it/s]\u001b[A\n",
            " 95% 1202/1269 [13:35<00:46,  1.43it/s]\u001b[A\n",
            " 95% 1203/1269 [13:35<00:45,  1.44it/s]\u001b[A\n",
            " 95% 1204/1269 [13:36<00:45,  1.44it/s]\u001b[A\n",
            " 95% 1205/1269 [13:37<00:44,  1.44it/s]\u001b[A\n",
            " 95% 1206/1269 [13:37<00:43,  1.44it/s]\u001b[A\n",
            " 95% 1207/1269 [13:38<00:42,  1.45it/s]\u001b[A\n",
            " 95% 1208/1269 [13:39<00:41,  1.45it/s]\u001b[A\n",
            " 95% 1209/1269 [13:40<00:41,  1.46it/s]\u001b[A\n",
            " 95% 1210/1269 [13:40<00:40,  1.46it/s]\u001b[A\n",
            " 95% 1211/1269 [13:41<00:39,  1.46it/s]\u001b[A\n",
            " 96% 1212/1269 [13:42<00:39,  1.46it/s]\u001b[A\n",
            " 96% 1213/1269 [13:42<00:38,  1.47it/s]\u001b[A\n",
            " 96% 1214/1269 [13:43<00:37,  1.47it/s]\u001b[A\n",
            " 96% 1215/1269 [13:44<00:36,  1.47it/s]\u001b[A\n",
            " 96% 1216/1269 [13:44<00:36,  1.47it/s]\u001b[A\n",
            " 96% 1217/1269 [13:45<00:35,  1.47it/s]\u001b[A\n",
            " 96% 1218/1269 [13:46<00:34,  1.46it/s]\u001b[A\n",
            " 96% 1219/1269 [13:46<00:34,  1.47it/s]\u001b[A\n",
            " 96% 1220/1269 [13:47<00:33,  1.47it/s]\u001b[A\n",
            " 96% 1221/1269 [13:48<00:32,  1.47it/s]\u001b[A\n",
            " 96% 1222/1269 [13:48<00:31,  1.47it/s]\u001b[A\n",
            " 96% 1223/1269 [13:49<00:31,  1.47it/s]\u001b[A\n",
            " 96% 1224/1269 [13:50<00:30,  1.47it/s]\u001b[A\n",
            " 97% 1225/1269 [13:50<00:29,  1.47it/s]\u001b[A\n",
            " 97% 1226/1269 [13:51<00:29,  1.47it/s]\u001b[A\n",
            " 97% 1227/1269 [13:52<00:28,  1.47it/s]\u001b[A\n",
            " 97% 1228/1269 [13:52<00:27,  1.47it/s]\u001b[A\n",
            " 97% 1229/1269 [13:53<00:27,  1.47it/s]\u001b[A\n",
            " 97% 1230/1269 [13:54<00:26,  1.47it/s]\u001b[A\n",
            " 97% 1231/1269 [13:54<00:25,  1.47it/s]\u001b[A\n",
            " 97% 1232/1269 [13:55<00:25,  1.47it/s]\u001b[A\n",
            " 97% 1233/1269 [13:56<00:24,  1.48it/s]\u001b[A\n",
            " 97% 1234/1269 [13:57<00:23,  1.47it/s]\u001b[A\n",
            " 97% 1235/1269 [13:57<00:23,  1.47it/s]\u001b[A\n",
            " 97% 1236/1269 [13:58<00:22,  1.47it/s]\u001b[A\n",
            " 97% 1237/1269 [13:59<00:21,  1.47it/s]\u001b[A\n",
            " 98% 1238/1269 [13:59<00:21,  1.47it/s]\u001b[A\n",
            " 98% 1239/1269 [14:00<00:20,  1.47it/s]\u001b[A\n",
            " 98% 1240/1269 [14:01<00:19,  1.47it/s]\u001b[A\n",
            " 98% 1241/1269 [14:01<00:19,  1.47it/s]\u001b[A\n",
            " 98% 1242/1269 [14:02<00:18,  1.47it/s]\u001b[A\n",
            " 98% 1243/1269 [14:03<00:17,  1.47it/s]\u001b[A\n",
            " 98% 1244/1269 [14:03<00:16,  1.47it/s]\u001b[A\n",
            " 98% 1245/1269 [14:04<00:16,  1.47it/s]\u001b[A\n",
            " 98% 1246/1269 [14:05<00:15,  1.47it/s]\u001b[A\n",
            " 98% 1247/1269 [14:05<00:14,  1.47it/s]\u001b[A\n",
            " 98% 1248/1269 [14:06<00:14,  1.47it/s]\u001b[A\n",
            " 98% 1249/1269 [14:07<00:13,  1.47it/s]\u001b[A\n",
            " 99% 1250/1269 [14:07<00:12,  1.47it/s]\u001b[A\n",
            " 99% 1251/1269 [14:08<00:12,  1.47it/s]\u001b[A\n",
            " 99% 1252/1269 [14:09<00:11,  1.47it/s]\u001b[A\n",
            " 99% 1253/1269 [14:09<00:10,  1.47it/s]\u001b[A\n",
            " 99% 1254/1269 [14:10<00:10,  1.47it/s]\u001b[A\n",
            " 99% 1255/1269 [14:11<00:09,  1.47it/s]\u001b[A\n",
            " 99% 1256/1269 [14:11<00:08,  1.47it/s]\u001b[A\n",
            " 99% 1257/1269 [14:12<00:08,  1.47it/s]\u001b[A\n",
            " 99% 1258/1269 [14:13<00:07,  1.47it/s]\u001b[A\n",
            " 99% 1259/1269 [14:13<00:06,  1.47it/s]\u001b[A\n",
            " 99% 1260/1269 [14:14<00:06,  1.47it/s]\u001b[A\n",
            " 99% 1261/1269 [14:15<00:05,  1.47it/s]\u001b[A\n",
            " 99% 1262/1269 [14:16<00:04,  1.46it/s]\u001b[A\n",
            "100% 1263/1269 [14:16<00:04,  1.47it/s]\u001b[A\n",
            "100% 1264/1269 [14:17<00:03,  1.47it/s]\u001b[A\n",
            "100% 1265/1269 [14:18<00:02,  1.46it/s]\u001b[A\n",
            "100% 1266/1269 [14:18<00:02,  1.47it/s]\u001b[A\n",
            "100% 1267/1269 [14:19<00:01,  1.46it/s]\u001b[A\n",
            "100% 1268/1269 [14:20<00:00,  1.47it/s]\u001b[A\n",
            "100% 1269/1269 [14:20<00:00,  1.77it/s]\u001b[A[INFO|trainer.py:726] 2022-11-04 16:55:57,747 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:55:57,748 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:55:57,749 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:55:57,749 >>   Batch size = 8\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 2.1513335704803467, 'eval_accuracy': 0.7320703864097595, 'eval_f1': 0.7320109890109889, 'eval_runtime': 5.63, 'eval_samples_per_second': 131.262, 'eval_steps_per_second': 16.519, 'epoch': 9.0}\n",
            "100% 93/93 [00:05<00:00, 16.45it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:56:03,380 >> Saving model checkpoint to models/ZeroShot/0/checkpoint-1269\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:56:03,381 >> Configuration saved in models/ZeroShot/0/checkpoint-1269/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:56:06,005 >> Model weights saved in models/ZeroShot/0/checkpoint-1269/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:56:06,006 >> tokenizer config file saved in models/ZeroShot/0/checkpoint-1269/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:56:06,006 >> Special tokens file saved in models/ZeroShot/0/checkpoint-1269/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 16:56:11,862 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1128] due to args.save_total_limit\n",
            "[INFO|trainer.py:1859] 2022-11-04 16:56:12,008 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1983] 2022-11-04 16:56:12,008 >> Loading best model from models/ZeroShot/0/checkpoint-282 (score: 0.7362268177803944).\n",
            "\n",
            "\u001b[A{'train_runtime': 878.223, 'train_samples_per_second': 46.024, 'train_steps_per_second': 1.445, 'train_loss': 0.04188857870462657, 'epoch': 9.0}\n",
            "\n",
            "100% 1269/1269 [14:38<00:00,  1.77it/s]\u001b[A[INFO|trainer.py:1893] 2022-11-04 16:56:15,527 >> Deleting older checkpoint [models/ZeroShot/0/checkpoint-1269] due to args.save_total_limit\n",
            "100% 1269/1269 [14:38<00:00,  1.44it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 16:56:15,772 >> Saving model checkpoint to models/ZeroShot/0/\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 16:56:15,774 >> Configuration saved in models/ZeroShot/0/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 16:56:18,365 >> Model weights saved in models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 16:56:18,366 >> tokenizer config file saved in models/ZeroShot/0/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 16:56:18,366 >> Special tokens file saved in models/ZeroShot/0/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.0419\n",
            "  train_runtime            = 0:14:38.22\n",
            "  train_samples            =       4491\n",
            "  train_samples_per_second =     46.024\n",
            "  train_steps_per_second   =      1.445\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-04 16:56:18,594 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:56:18,596 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:56:18,596 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:56:18,596 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 16.97it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =     0.7375\n",
            "  eval_f1                 =     0.7362\n",
            "  eval_loss               =     0.9461\n",
            "  eval_runtime            = 0:00:05.56\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    132.835\n",
            "  eval_steps_per_second   =     16.717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "X0305tDdt5ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "id": "QQj2MaudbdDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create save path\n",
        "!mkdir -p /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/ZeroShot/0/* /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/"
      ],
      "metadata": {
        "id": "8kZBemeibkew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Bring back saved model here. \n",
        "#!mkdir -p /content/models/ZeroShot/0/\n",
        "# !cp -r /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/ZeroShot/0/* /content/models/ZeroShot/0/"
      ],
      "metadata": {
        "id": "0PfShC0RbltP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on dev"
      ],
      "metadata": {
        "id": "JEowe57gt_la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fix run_glue_f1_macro\n",
        "# remove_columns_() doesn't exist anymore so change to remove_columns()\n",
        "\n",
        "!cp /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.txt\n",
        "\n",
        "with open('/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.txt', 'r+') as f:\n",
        "    text = f.read()\n",
        "    text = text.replace('remove_columns_(', 'remove_columns(')\n",
        "with open('/content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.txt', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "!cp /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.txt /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py\n",
        "!rm /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.txt"
      ],
      "metadata": {
        "id": "XClVFvSMbyZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/ZeroShot/0' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/ZeroShot/0/eval-dev/ \\\n",
        "    \t--seed 0 \\\n",
        "    \t--train_file      Data/ZeroShot/train.csv \\\n",
        "    \t--validation_file Data/ZeroShot/dev.csv \\\n",
        "      --test_file Data/ZeroShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgopwr7Jbmzg",
        "outputId": "a093b999-15f0-4ae1-de45-b21dc388be7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/ZeroShot/0/eval-dev/runs/Nov04_16-57-41_e5fee68aa035,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/ZeroShot/0/eval-dev/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/ZeroShot/0/eval-dev/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=0,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: Data/ZeroShot/train.csv\n",
            "INFO:__main__:load a local file for validation: Data/ZeroShot/dev.csv\n",
            "INFO:__main__:load a local file for test: Data/ZeroShot/dev.csv\n",
            "WARNING:datasets.builder:Using custom data configuration default-d01729355ec5597a\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-d01729355ec5597a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "\r  0% 0/3 [00:00<?, ?it/s]\r100% 3/3 [00:00<00:00, 228.03it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-04 16:57:42,491 >> loading configuration file /content/models/ZeroShot/0/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-04 16:57:42,497 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/ZeroShot/0\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 16:57:42,498 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 16:57:42,498 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 16:57:42,498 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 16:57:42,498 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 16:57:42,498 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2155] 2022-11-04 16:57:42,646 >> loading weights file /content/models/ZeroShot/0/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2608] 2022-11-04 16:57:44,629 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:2617] 2022-11-04 16:57:44,629 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/models/ZeroShot/0.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d01729355ec5597a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a30c4d488c372a02.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d01729355ec5597a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-dd7189aa79851f38.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-d01729355ec5597a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a5fff609339c153c.arrow\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-04 16:57:48,765 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:57:48,766 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:57:48,767 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:57:48,767 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 17.89it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.7375\n",
            "  eval_f1                 =     0.7362\n",
            "  eval_loss               =     0.9461\n",
            "  eval_runtime            = 0:00:06.25\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    118.095\n",
            "  eval_steps_per_second   =     14.862\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:726] 2022-11-04 16:57:55,028 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 16:57:55,030 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 16:57:55,030 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 16:57:55,030 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 17.89it/s]\n",
            "INFO:__main__:***** Test results None *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create submission file for dev data\n",
        "\n",
        "def insert_to_submission_file( submission_format_file, input_file, prediction_format_file, setting ) :\n",
        "    submission_header, submission_content = load_csv( submission_format_file )\n",
        "    input_header     , input_data         = load_csv( input_file             )\n",
        "    prediction_header, prediction_data    = load_csv( prediction_format_file, '\\t' )\n",
        "\n",
        "    assert len( input_data ) == len( prediction_data )\n",
        "\n",
        "    ## submission_header ['ID', 'Language', 'Setting', 'Label']\n",
        "    ## input_header      ['label', 'sentence1' ]\n",
        "    ## prediction_header ['index', 'prediction']\n",
        "\n",
        "    prediction_data = list( reversed( prediction_data ) )\n",
        "\n",
        "    started_insert  = False\n",
        "    for elem in submission_content : \n",
        "        if elem[ submission_header.index( 'Setting' ) ] != setting :\n",
        "            if started_insert :\n",
        "                if len( prediction_data ) == 0 :\n",
        "                    break\n",
        "                else : \n",
        "                    raise Exception( \"Update should to contiguous ... something wrong.\" ) \n",
        "            continue\n",
        "        started_insert = True\n",
        "        elem[ submission_header.index( 'Label' ) ] = prediction_data.pop()[ prediction_header.index( 'prediction' ) ]\n",
        "\n",
        "    return [ submission_header ] + submission_content\n",
        "\n",
        "params = {\n",
        "    'submission_format_file' : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev_submission_format.csv' ,\n",
        "    'input_file'             : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/ZeroShot/0/eval-dev/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'zero_shot'\n",
        "\n",
        "updated_data = insert_to_submission_file( **params )\n",
        "\n",
        "!mkdir -p outputs\n",
        "\n",
        "write_csv( updated_data, 'outputs/zero_shot_dev_formated.csv' ) \n",
        "\n",
        "# run eval script for dev data\n",
        "\n",
        "import sys\n",
        "sys.path.append( '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/' ) \n",
        "from SubTask1Evaluator import evaluate_submission\n",
        "\n",
        "\n",
        "submission_file = 'outputs/zero_shot_dev_formated.csv'\n",
        "gold_file       = '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev_gold.csv'\n",
        "\n",
        "results = evaluate_submission( submission_file, gold_file )\n",
        "%reload_ext google.colab.data_table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data=results[1:], columns=results[0])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "V35BA20KgdEh",
        "outputId": "3161c571-41a6-42b0-b2e8-868ee37d1866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/zero_shot_dev_formated.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Settings Languages    F1 Score (Macro)\n",
              "0  zero_shot        EN             0.71835\n",
              "1  zero_shot        PT            0.637363\n",
              "2  zero_shot     EN,PT            0.705875\n",
              "3   one_shot        EN  (None, None, None)\n",
              "4   one_shot        PT  (None, None, None)\n",
              "5   one_shot     EN,PT  (None, None, None)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abe5eefd-1695-4073-90ce-2bb4387432e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Settings</th>\n",
              "      <th>Languages</th>\n",
              "      <th>F1 Score (Macro)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.71835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.637363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.705875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>(None, None, None)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>(None, None, None)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>(None, None, None)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abe5eefd-1695-4073-90ce-2bb4387432e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-abe5eefd-1695-4073-90ce-2bb4387432e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-abe5eefd-1695-4073-90ce-2bb4387432e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a8bd4d5e58f96183/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_shot\",\n\"EN\",\n0.7183498489001335],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"zero_shot\",\n\"PT\",\n0.6373626373626374],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"zero_shot\",\n\"EN,PT\",\n0.705875232245273],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"one_shot\",\n\"EN\",\n[null, null, null]],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"one_shot\",\n\"PT\",\n[null, null, null]],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"one_shot\",\n\"EN,PT\",\n[null, null, null]]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Settings\"], [\"string\", \"Languages\"], [\"string\", \"F1 Score (Macro)\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Shot"
      ],
      "metadata": {
        "id": "ff5SZ0PklYPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "53LhwFnjuCnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train one shot\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path 'bert-base-multilingual-cased' \\\n",
        "    \t--do_train \\\n",
        "    \t--do_eval \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddtx1fNflac8",
        "outputId": "bb3dbc22-d47d-4928-c391-74a185e54bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/runs/Nov04_17-09-52_e5fee68aa035,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/OneShot/1/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: Data/OneShot/train.csv\n",
            "INFO:__main__:load a local file for validation: Data/OneShot/dev.csv\n",
            "WARNING:datasets.builder:Using custom data configuration default-9dad7fd63ba28b7e\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-9dad7fd63ba28b7e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "\rDownloading data files:   0% 0/2 [00:00<?, ?it/s]\rDownloading data files: 100% 2/2 [00:00<00:00, 9310.33it/s]\n",
            "\rExtracting data files:   0% 0/2 [00:00<?, ?it/s]\rExtracting data files: 100% 2/2 [00:00<00:00, 1213.63it/s]\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \rDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-9dad7fd63ba28b7e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 945.41it/s]\n",
            "[INFO|configuration_utils.py:654] 2022-11-04 17:09:52,892 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-04 17:09:52,894 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2022-11-04 17:09:52,929 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-04 17:09:52,930 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 17:09:52,930 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 17:09:52,931 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 17:09:52,931 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 17:09:52,931 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1775] 2022-11-04 17:09:52,931 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2022-11-04 17:09:52,931 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-04 17:09:52,932 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2158] 2022-11-04 17:09:53,067 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/pytorch_model.bin\n",
            "[WARNING|modeling_utils.py:2599] 2022-11-04 17:09:57,267 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2611] 2022-11-04 17:09:57,267 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 80% 4/5 [00:00<00:00,  4.24ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]\n",
            "INFO:__main__:Sample 1100 of the training set: {'label': 0, 'sentence1': 'Given how many hours of beauty sleep most kitties like to clock up, choosing the right place for them to lay their heads is critical.', 'sentence2': 'beauty sleep.\\nThe qualities that give pleasure to the senses. A very attractive or seductive looking woman. A natural and periodic state of rest during which consciousness of the world is suspended. A torpid state resembling deep sleep', 'input_ids': [101, 90491, 14796, 11299, 19573, 10108, 54883, 63658, 10992, 72812, 14197, 11850, 10114, 52843, 10741, 117, 11257, 90739, 10105, 13448, 11192, 10142, 11345, 10114, 47413, 10455, 42399, 10124, 24523, 119, 102, 54883, 63658, 119, 10117, 15510, 14197, 10189, 18090, 20648, 42658, 12101, 10114, 10105, 15495, 10107, 119, 138, 12558, 102512, 10345, 38559, 43805, 14079, 34279, 18299, 119, 138, 13409, 10111, 107491, 10350, 11388, 10108, 17333, 10939, 10319, 91448, 10108, 10105, 11356, 10124, 49799, 119, 138, 10114, 33394, 11249, 11388, 39429, 10451, 38245, 26591, 63658, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 516 of the training set: {'label': 0, 'sentence1': 'Yet Sergeant Mark Brady, who oversees major collision investigations for South Yorkshire Police, told the inquiry, “Had there been a hard shoulder, had Jason and Alexandru pulled on to the hard shoulder, my opinion is that Mr Szuba would have driven clean past them.”', 'sentence2': 'hard shoulder.\\nNot easy; requiring great physical or mental effort to accomplish or comprehend or endure. Dispassionate. The part of the body between the neck and the upper arm. A cut of meat including the upper joint of the foreleg', 'input_ids': [101, 71547, 54118, 11997, 45982, 117, 10479, 10491, 20262, 10107, 11922, 94460, 87748, 10142, 11056, 27577, 18051, 117, 21937, 10105, 10106, 56914, 117, 100, 66434, 11155, 10590, 169, 19118, 78681, 117, 10374, 16796, 10111, 43816, 65884, 10135, 10114, 10105, 19118, 78681, 117, 15127, 32282, 10124, 10189, 12916, 156, 13078, 10537, 10894, 10529, 39803, 55911, 17781, 11345, 119, 100, 102, 19118, 78681, 119, 16040, 44346, 132, 74063, 14772, 22899, 10345, 27993, 24912, 10114, 13621, 22530, 62932, 13264, 10345, 10212, 30619, 14786, 10162, 10345, 11572, 12101, 119, 101270, 36388, 27719, 10216, 119, 10117, 10668, 10108, 10105, 14333, 10948, 10105, 63938, 10111, 10105, 24172, 31251, 119, 138, 21610, 10108, 64080, 11198, 10105, 24172, 25680, 10108, 10105, 10142, 105687, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "INFO:__main__:Sample 2089 of the training set: {'label': 0, 'sentence1': 'While the family fun hub holds back from turning into a ghost town for Japanese Yokai (demons) to roam around this year, Downtown East is bringing its scary stories online with a series of episodes on Instagram Stories.', 'sentence2': 'ghost town.\\nA mental representation of some haunting experience. A writer who gives the credit of authorship to someone else. An urban area with a fixed boundary that is smaller than a city. The people living in a municipality smaller than a city', 'input_ids': [101, 14600, 10105, 11365, 41807, 65896, 28278, 12014, 10188, 48448, 10708, 169, 100766, 12221, 10142, 13847, 30665, 18511, 113, 30776, 10891, 114, 10114, 25470, 11008, 12166, 10531, 10924, 117, 68339, 11830, 10124, 45749, 10474, 187, 15983, 10157, 21158, 13893, 10169, 169, 11366, 10108, 23604, 10135, 83019, 25955, 119, 102, 100766, 12221, 119, 138, 27993, 43847, 10108, 11152, 20091, 70925, 20627, 119, 138, 17556, 10479, 24952, 10105, 37021, 10108, 39476, 17883, 10114, 30455, 40843, 119, 10313, 23351, 11168, 10169, 169, 37770, 42584, 10189, 10124, 23309, 11084, 169, 11584, 119, 10117, 11426, 14625, 10106, 169, 17288, 23309, 11084, 169, 11584, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:726] 2022-11-04 17:10:00,645 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1608] 2022-11-04 17:10:00,651 >> ***** Running training *****\n",
            "[INFO|trainer.py:1609] 2022-11-04 17:10:00,651 >>   Num examples = 4631\n",
            "[INFO|trainer.py:1610] 2022-11-04 17:10:00,651 >>   Num Epochs = 9\n",
            "[INFO|trainer.py:1611] 2022-11-04 17:10:00,651 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1612] 2022-11-04 17:10:00,651 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1613] 2022-11-04 17:10:00,651 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1614] 2022-11-04 17:10:00,651 >>   Total optimization steps = 1305\n",
            "[INFO|trainer.py:1616] 2022-11-04 17:10:00,652 >>   Number of trainable parameters = 177854978\n",
            " 11% 145/1305 [01:33<11:50,  1.63it/s][INFO|trainer.py:726] 2022-11-04 17:11:34,415 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:11:34,417 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:11:34,417 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:11:34,417 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.90it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.57it/s]\u001b[A\n",
            " 10% 9/93 [00:00<00:04, 18.09it/s]\u001b[A\n",
            " 12% 11/93 [00:00<00:04, 17.89it/s]\u001b[A\n",
            " 14% 13/93 [00:00<00:04, 17.71it/s]\u001b[A\n",
            " 16% 15/93 [00:00<00:04, 17.46it/s]\u001b[A\n",
            " 18% 17/93 [00:00<00:04, 17.24it/s]\u001b[A\n",
            " 20% 19/93 [00:01<00:04, 17.33it/s]\u001b[A\n",
            " 23% 21/93 [00:01<00:04, 17.31it/s]\u001b[A\n",
            " 25% 23/93 [00:01<00:04, 17.20it/s]\u001b[A\n",
            " 27% 25/93 [00:01<00:03, 17.22it/s]\u001b[A\n",
            " 29% 27/93 [00:01<00:03, 17.25it/s]\u001b[A\n",
            " 31% 29/93 [00:01<00:03, 17.27it/s]\u001b[A\n",
            " 33% 31/93 [00:01<00:03, 17.23it/s]\u001b[A\n",
            " 35% 33/93 [00:01<00:03, 17.24it/s]\u001b[A\n",
            " 38% 35/93 [00:01<00:03, 17.25it/s]\u001b[A\n",
            " 40% 37/93 [00:02<00:03, 17.22it/s]\u001b[A\n",
            " 42% 39/93 [00:02<00:03, 17.30it/s]\u001b[A\n",
            " 44% 41/93 [00:02<00:03, 17.30it/s]\u001b[A\n",
            " 46% 43/93 [00:02<00:02, 17.23it/s]\u001b[A\n",
            " 48% 45/93 [00:02<00:02, 17.21it/s]\u001b[A\n",
            " 51% 47/93 [00:02<00:02, 17.27it/s]\u001b[A\n",
            " 53% 49/93 [00:02<00:02, 17.25it/s]\u001b[A\n",
            " 55% 51/93 [00:02<00:02, 17.18it/s]\u001b[A\n",
            " 57% 53/93 [00:03<00:02, 17.21it/s]\u001b[A\n",
            " 59% 55/93 [00:03<00:02, 17.26it/s]\u001b[A\n",
            " 61% 57/93 [00:03<00:02, 17.26it/s]\u001b[A\n",
            " 63% 59/93 [00:03<00:01, 17.25it/s]\u001b[A\n",
            " 66% 61/93 [00:03<00:01, 17.33it/s]\u001b[A\n",
            " 68% 63/93 [00:03<00:01, 17.34it/s]\u001b[A\n",
            " 70% 65/93 [00:03<00:01, 17.28it/s]\u001b[A\n",
            " 72% 67/93 [00:03<00:01, 17.30it/s]\u001b[A\n",
            " 74% 69/93 [00:03<00:01, 17.28it/s]\u001b[A\n",
            " 76% 71/93 [00:04<00:01, 17.23it/s]\u001b[A\n",
            " 78% 73/93 [00:04<00:01, 17.20it/s]\u001b[A\n",
            " 81% 75/93 [00:04<00:01, 17.28it/s]\u001b[A\n",
            " 83% 77/93 [00:04<00:00, 17.32it/s]\u001b[A\n",
            " 85% 79/93 [00:04<00:00, 17.21it/s]\u001b[A\n",
            " 87% 81/93 [00:04<00:00, 17.27it/s]\u001b[A\n",
            " 89% 83/93 [00:04<00:00, 17.24it/s]\u001b[A\n",
            " 91% 85/93 [00:04<00:00, 17.19it/s]\u001b[A\n",
            " 94% 87/93 [00:05<00:00, 17.11it/s]\u001b[A\n",
            " 96% 89/93 [00:05<00:00, 17.15it/s]\u001b[A\n",
            " 98% 91/93 [00:05<00:00, 17.20it/s]\u001b[A\n",
            "{'eval_loss': 0.7318678498268127, 'eval_accuracy': 0.7334235310554504, 'eval_f1': 0.7329227642052698, 'eval_runtime': 5.3682, 'eval_samples_per_second': 137.662, 'eval_steps_per_second': 17.324, 'epoch': 1.0}\n",
            "\n",
            " 11% 145/1305 [01:39<11:50,  1.63it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:11:39,787 >> Saving model checkpoint to models/OneShot/1/checkpoint-145\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:11:39,788 >> Configuration saved in models/OneShot/1/checkpoint-145/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:11:42,542 >> Model weights saved in models/OneShot/1/checkpoint-145/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:11:42,542 >> tokenizer config file saved in models/OneShot/1/checkpoint-145/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:11:42,542 >> Special tokens file saved in models/OneShot/1/checkpoint-145/special_tokens_map.json\n",
            " 22% 290/1305 [03:24<10:34,  1.60it/s][INFO|trainer.py:726] 2022-11-04 17:13:25,119 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:13:25,121 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:13:25,121 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:13:25,121 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.77it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.09it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.98it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.64it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.30it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 17.08it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.66it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.73it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.71it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.62it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.56it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.64it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.62it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.70it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.74it/s]\u001b[A\n",
            " 37% 34/93 [00:01<00:03, 16.84it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.89it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.83it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.77it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.69it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.68it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.72it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.75it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.80it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.78it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.73it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.74it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.74it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.55it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.40it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.40it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.43it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.45it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.56it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.65it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.79it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.79it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.70it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.70it/s]\u001b[A\n",
            " 90% 84/93 [00:04<00:00, 16.68it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.58it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.59it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.58it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.57it/s]\u001b[A\n",
            "{'eval_loss': 0.7427625060081482, 'eval_accuracy': 0.798376202583313, 'eval_f1': 0.7982283987605389, 'eval_runtime': 5.552, 'eval_samples_per_second': 133.104, 'eval_steps_per_second': 16.751, 'epoch': 2.0}\n",
            "\n",
            " 22% 290/1305 [03:30<10:34,  1.60it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:13:30,674 >> Saving model checkpoint to models/OneShot/1/checkpoint-290\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:13:30,675 >> Configuration saved in models/OneShot/1/checkpoint-290/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:13:33,139 >> Model weights saved in models/OneShot/1/checkpoint-290/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:13:33,140 >> tokenizer config file saved in models/OneShot/1/checkpoint-290/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:13:33,140 >> Special tokens file saved in models/OneShot/1/checkpoint-290/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 17:13:38,862 >> Deleting older checkpoint [models/OneShot/1/checkpoint-145] due to args.save_total_limit\n",
            " 33% 435/1305 [05:17<09:12,  1.58it/s][INFO|trainer.py:726] 2022-11-04 17:15:18,101 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:15:18,103 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:15:18,103 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:15:18,103 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.37it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.55it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.70it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.38it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.18it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.91it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.44it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.71it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.48it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.51it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.52it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.52it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.44it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.26it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.20it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 15.92it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.40it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.51it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.61it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.64it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.52it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.30it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.25it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.31it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.39it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.39it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.17it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 16.41it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.59it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.54it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.52it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.38it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.30it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.31it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.41it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.57it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.51it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.37it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.34it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.36it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.34it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.38it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.43it/s]\u001b[A\n",
            "{'eval_loss': 0.9530001878738403, 'eval_accuracy': 0.7997293472290039, 'eval_f1': 0.7993763298848044, 'eval_runtime': 5.6299, 'eval_samples_per_second': 131.264, 'eval_steps_per_second': 16.519, 'epoch': 3.0}\n",
            "\n",
            " 33% 435/1305 [05:23<09:12,  1.58it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:15:23,741 >> Saving model checkpoint to models/OneShot/1/checkpoint-435\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:15:23,742 >> Configuration saved in models/OneShot/1/checkpoint-435/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:15:26,322 >> Model weights saved in models/OneShot/1/checkpoint-435/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:15:26,323 >> tokenizer config file saved in models/OneShot/1/checkpoint-435/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:15:26,324 >> Special tokens file saved in models/OneShot/1/checkpoint-435/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 17:15:32,107 >> Deleting older checkpoint [models/OneShot/1/checkpoint-290] due to args.save_total_limit\n",
            "{'loss': 0.1666, 'learning_rate': 1.2337164750957855e-05, 'epoch': 3.45}\n",
            " 44% 580/1305 [07:10<07:40,  1.57it/s][INFO|trainer.py:726] 2022-11-04 17:17:11,453 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:17:11,455 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:17:11,455 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:17:11,455 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.02it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.34it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.75it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.42it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.03it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.82it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.52it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.69it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.59it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.58it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.52it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.33it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.23it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.34it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.46it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.50it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.49it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.46it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.41it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.38it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.35it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.36it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.38it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.44it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.43it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.45it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 16.43it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.39it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.38it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.33it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.34it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.33it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.37it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.46it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.45it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.46it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.46it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.43it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.33it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.30it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.40it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.36it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.36it/s]\u001b[A\n",
            "{'eval_loss': 0.7490686774253845, 'eval_accuracy': 0.817320704460144, 'eval_f1': 0.8161884600290739, 'eval_runtime': 5.6339, 'eval_samples_per_second': 131.17, 'eval_steps_per_second': 16.507, 'epoch': 4.0}\n",
            "\n",
            " 44% 580/1305 [07:16<07:40,  1.57it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:17:17,091 >> Saving model checkpoint to models/OneShot/1/checkpoint-580\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:17:17,092 >> Configuration saved in models/OneShot/1/checkpoint-580/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:17:20,080 >> Model weights saved in models/OneShot/1/checkpoint-580/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:17:20,081 >> tokenizer config file saved in models/OneShot/1/checkpoint-580/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:17:20,082 >> Special tokens file saved in models/OneShot/1/checkpoint-580/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 17:17:26,415 >> Deleting older checkpoint [models/OneShot/1/checkpoint-435] due to args.save_total_limit\n",
            " 56% 725/1305 [09:05<06:10,  1.56it/s][INFO|trainer.py:726] 2022-11-04 17:19:05,948 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:19:05,950 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:19:05,950 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:19:05,950 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 23.16it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.36it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.72it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.46it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.12it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.84it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.44it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.67it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.64it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.53it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.57it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.62it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.42it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.21it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.35it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.48it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.54it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.54it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.55it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.49it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.39it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.24it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.32it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.43it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.50it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.52it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.57it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.53it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.44it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.29it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.26it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.35it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.47it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.53it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.52it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.43it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.36it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.28it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.27it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.40it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.44it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.50it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.51it/s]\u001b[A\n",
            "{'eval_loss': 0.9389731287956238, 'eval_accuracy': 0.8267929553985596, 'eval_f1': 0.8257563477210097, 'eval_runtime': 5.6222, 'eval_samples_per_second': 131.444, 'eval_steps_per_second': 16.542, 'epoch': 5.0}\n",
            "\n",
            " 56% 725/1305 [09:10<06:10,  1.56it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:19:11,575 >> Saving model checkpoint to models/OneShot/1/checkpoint-725\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:19:11,576 >> Configuration saved in models/OneShot/1/checkpoint-725/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:19:14,143 >> Model weights saved in models/OneShot/1/checkpoint-725/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:19:14,144 >> tokenizer config file saved in models/OneShot/1/checkpoint-725/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:19:14,144 >> Special tokens file saved in models/OneShot/1/checkpoint-725/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 17:19:19,848 >> Deleting older checkpoint [models/OneShot/1/checkpoint-580] due to args.save_total_limit\n",
            " 67% 870/1305 [10:59<04:37,  1.57it/s][INFO|trainer.py:726] 2022-11-04 17:20:59,787 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:20:59,789 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:20:59,790 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:20:59,790 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.09it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.73it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.93it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.52it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.24it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 17.02it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.47it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.71it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.57it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.52it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.40it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.51it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.39it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.27it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.40it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.48it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.50it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.44it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.39it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.43it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.35it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.27it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.39it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.42it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.41it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.47it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.44it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 16.43it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.31it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.23it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.33it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.37it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.43it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.43it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.44it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.40it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.38it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.34it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.36it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.45it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.53it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.55it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.44it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.32it/s]\u001b[A\n",
            "{'eval_loss': 1.0550755262374878, 'eval_accuracy': 0.8389715552330017, 'eval_f1': 0.8385446787729242, 'eval_runtime': 5.6289, 'eval_samples_per_second': 131.287, 'eval_steps_per_second': 16.522, 'epoch': 6.0}\n",
            "\n",
            " 67% 870/1305 [11:04<04:37,  1.57it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:21:05,422 >> Saving model checkpoint to models/OneShot/1/checkpoint-870\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:21:05,423 >> Configuration saved in models/OneShot/1/checkpoint-870/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:21:08,133 >> Model weights saved in models/OneShot/1/checkpoint-870/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:21:08,134 >> tokenizer config file saved in models/OneShot/1/checkpoint-870/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:21:08,134 >> Special tokens file saved in models/OneShot/1/checkpoint-870/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 17:21:14,791 >> Deleting older checkpoint [models/OneShot/1/checkpoint-725] due to args.save_total_limit\n",
            "{'loss': 0.0196, 'learning_rate': 4.674329501915709e-06, 'epoch': 6.9}\n",
            " 78% 1015/1305 [12:53<03:04,  1.58it/s][INFO|trainer.py:726] 2022-11-04 17:22:54,296 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:22:54,297 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:22:54,298 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:22:54,298 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 25.10it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 19.07it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 18.07it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.63it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.15it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.94it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.65it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.76it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.77it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.57it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.53it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.52it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.52it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.39it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.32it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.41it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.51it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.47it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.50it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.44it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.48it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.41it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.37it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.39it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.41it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.40it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.47it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.42it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 16.39it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.39it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.34it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.40it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.35it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.44it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.47it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.56it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.43it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.45it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.39it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.42it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.32it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.39it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.50it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1133246421813965, 'eval_accuracy': 0.8335588574409485, 'eval_f1': 0.8329114622108252, 'eval_runtime': 5.6131, 'eval_samples_per_second': 131.656, 'eval_steps_per_second': 16.568, 'epoch': 7.0}\n",
            " 78% 1015/1305 [12:59<03:04,  1.58it/s]\n",
            "100% 93/93 [00:05<00:00, 16.52it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:22:59,912 >> Saving model checkpoint to models/OneShot/1/checkpoint-1015\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:22:59,913 >> Configuration saved in models/OneShot/1/checkpoint-1015/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:23:02,534 >> Model weights saved in models/OneShot/1/checkpoint-1015/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:23:02,535 >> tokenizer config file saved in models/OneShot/1/checkpoint-1015/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:23:02,535 >> Special tokens file saved in models/OneShot/1/checkpoint-1015/special_tokens_map.json\n",
            " 89% 1160/1305 [14:47<01:32,  1.56it/s][INFO|trainer.py:726] 2022-11-04 17:24:47,821 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:24:47,823 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:24:47,823 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:24:47,823 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.15it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.73it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.82it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.47it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 17.10it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.86it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.50it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.62it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.65it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.67it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.63it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.56it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.60it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.42it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.27it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.30it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.39it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.53it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.61it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.53it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.55it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.30it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.32it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.28it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.40it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.45it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.55it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.55it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:01, 16.55it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.40it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.26it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.23it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.32it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.36it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.48it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.53it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.50it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.45it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.35it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.28it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.25it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.33it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.45it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.53it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.58it/s]\u001b[A\n",
            "{'eval_loss': 1.1312658786773682, 'eval_accuracy': 0.8349120616912842, 'eval_f1': 0.8341220194289078, 'eval_runtime': 5.6193, 'eval_samples_per_second': 131.51, 'eval_steps_per_second': 16.55, 'epoch': 8.0}\n",
            "\n",
            " 89% 1160/1305 [14:52<01:32,  1.56it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:24:53,444 >> Saving model checkpoint to models/OneShot/1/checkpoint-1160\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:24:53,445 >> Configuration saved in models/OneShot/1/checkpoint-1160/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:24:56,080 >> Model weights saved in models/OneShot/1/checkpoint-1160/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:24:56,080 >> tokenizer config file saved in models/OneShot/1/checkpoint-1160/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:24:56,082 >> Special tokens file saved in models/OneShot/1/checkpoint-1160/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 17:25:02,379 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1015] due to args.save_total_limit\n",
            "100% 1305/1305 [16:41<00:00,  1.57it/s][INFO|trainer.py:726] 2022-11-04 17:26:42,427 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:26:42,428 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:26:42,429 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:26:42,429 >>   Batch size = 8\n",
            "\n",
            "  0% 0/93 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 3/93 [00:00<00:03, 24.38it/s]\u001b[A\n",
            "  6% 6/93 [00:00<00:04, 18.41it/s]\u001b[A\n",
            "  9% 8/93 [00:00<00:04, 17.62it/s]\u001b[A\n",
            " 11% 10/93 [00:00<00:04, 17.28it/s]\u001b[A\n",
            " 13% 12/93 [00:00<00:04, 16.93it/s]\u001b[A\n",
            " 15% 14/93 [00:00<00:04, 16.71it/s]\u001b[A\n",
            " 17% 16/93 [00:00<00:04, 16.44it/s]\u001b[A\n",
            " 19% 18/93 [00:01<00:04, 16.59it/s]\u001b[A\n",
            " 22% 20/93 [00:01<00:04, 16.47it/s]\u001b[A\n",
            " 24% 22/93 [00:01<00:04, 16.46it/s]\u001b[A\n",
            " 26% 24/93 [00:01<00:04, 16.54it/s]\u001b[A\n",
            " 28% 26/93 [00:01<00:04, 16.53it/s]\u001b[A\n",
            " 30% 28/93 [00:01<00:03, 16.46it/s]\u001b[A\n",
            " 32% 30/93 [00:01<00:03, 16.34it/s]\u001b[A\n",
            " 34% 32/93 [00:01<00:03, 16.35it/s]\u001b[A\n",
            " 37% 34/93 [00:02<00:03, 16.38it/s]\u001b[A\n",
            " 39% 36/93 [00:02<00:03, 16.42it/s]\u001b[A\n",
            " 41% 38/93 [00:02<00:03, 16.46it/s]\u001b[A\n",
            " 43% 40/93 [00:02<00:03, 16.54it/s]\u001b[A\n",
            " 45% 42/93 [00:02<00:03, 16.54it/s]\u001b[A\n",
            " 47% 44/93 [00:02<00:02, 16.47it/s]\u001b[A\n",
            " 49% 46/93 [00:02<00:02, 16.41it/s]\u001b[A\n",
            " 52% 48/93 [00:02<00:02, 16.32it/s]\u001b[A\n",
            " 54% 50/93 [00:02<00:02, 16.34it/s]\u001b[A\n",
            " 56% 52/93 [00:03<00:02, 16.33it/s]\u001b[A\n",
            " 58% 54/93 [00:03<00:02, 16.45it/s]\u001b[A\n",
            " 60% 56/93 [00:03<00:02, 16.56it/s]\u001b[A\n",
            " 62% 58/93 [00:03<00:02, 16.53it/s]\u001b[A\n",
            " 65% 60/93 [00:03<00:02, 16.47it/s]\u001b[A\n",
            " 67% 62/93 [00:03<00:01, 16.40it/s]\u001b[A\n",
            " 69% 64/93 [00:03<00:01, 16.32it/s]\u001b[A\n",
            " 71% 66/93 [00:03<00:01, 16.28it/s]\u001b[A\n",
            " 73% 68/93 [00:04<00:01, 16.35it/s]\u001b[A\n",
            " 75% 70/93 [00:04<00:01, 16.37it/s]\u001b[A\n",
            " 77% 72/93 [00:04<00:01, 16.45it/s]\u001b[A\n",
            " 80% 74/93 [00:04<00:01, 16.52it/s]\u001b[A\n",
            " 82% 76/93 [00:04<00:01, 16.51it/s]\u001b[A\n",
            " 84% 78/93 [00:04<00:00, 16.36it/s]\u001b[A\n",
            " 86% 80/93 [00:04<00:00, 16.28it/s]\u001b[A\n",
            " 88% 82/93 [00:04<00:00, 16.30it/s]\u001b[A\n",
            " 90% 84/93 [00:05<00:00, 16.37it/s]\u001b[A\n",
            " 92% 86/93 [00:05<00:00, 16.42it/s]\u001b[A\n",
            " 95% 88/93 [00:05<00:00, 16.48it/s]\u001b[A\n",
            " 97% 90/93 [00:05<00:00, 16.56it/s]\u001b[A\n",
            " 99% 92/93 [00:05<00:00, 16.54it/s]\u001b[A\n",
            "{'eval_loss': 1.135117769241333, 'eval_accuracy': 0.8362652063369751, 'eval_f1': 0.8356283490041452, 'eval_runtime': 5.6319, 'eval_samples_per_second': 131.216, 'eval_steps_per_second': 16.513, 'epoch': 9.0}\n",
            "\n",
            "100% 1305/1305 [16:47<00:00,  1.57it/s]\n",
            "                                   \u001b[A[INFO|trainer.py:2678] 2022-11-04 17:26:48,062 >> Saving model checkpoint to models/OneShot/1/checkpoint-1305\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:26:48,063 >> Configuration saved in models/OneShot/1/checkpoint-1305/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:26:50,721 >> Model weights saved in models/OneShot/1/checkpoint-1305/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:26:50,722 >> tokenizer config file saved in models/OneShot/1/checkpoint-1305/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:26:50,722 >> Special tokens file saved in models/OneShot/1/checkpoint-1305/special_tokens_map.json\n",
            "[INFO|trainer.py:2756] 2022-11-04 17:26:56,726 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1160] due to args.save_total_limit\n",
            "[INFO|trainer.py:1859] 2022-11-04 17:26:56,880 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1983] 2022-11-04 17:26:56,880 >> Loading best model from models/OneShot/1/checkpoint-870 (score: 0.8385446787729242).\n",
            "{'train_runtime': 1020.1714, 'train_samples_per_second': 40.855, 'train_steps_per_second': 1.279, 'train_loss': 0.07190767343930357, 'epoch': 9.0}\n",
            "100% 1305/1305 [17:00<00:00,  1.57it/s][INFO|trainer.py:1893] 2022-11-04 17:27:00,825 >> Deleting older checkpoint [models/OneShot/1/checkpoint-1305] due to args.save_total_limit\n",
            "100% 1305/1305 [17:00<00:00,  1.28it/s]\n",
            "[INFO|trainer.py:2678] 2022-11-04 17:27:01,109 >> Saving model checkpoint to models/OneShot/1/\n",
            "[INFO|configuration_utils.py:447] 2022-11-04 17:27:01,110 >> Configuration saved in models/OneShot/1/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-11-04 17:27:03,746 >> Model weights saved in models/OneShot/1/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2125] 2022-11-04 17:27:03,746 >> tokenizer config file saved in models/OneShot/1/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2132] 2022-11-04 17:27:03,747 >> Special tokens file saved in models/OneShot/1/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        9.0\n",
            "  train_loss               =     0.0719\n",
            "  train_runtime            = 0:17:00.17\n",
            "  train_samples            =       4631\n",
            "  train_samples_per_second =     40.855\n",
            "  train_steps_per_second   =      1.279\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-04 17:27:04,018 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:27:04,019 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:27:04,019 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:27:04,019 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 16.90it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        9.0\n",
            "  eval_accuracy           =      0.839\n",
            "  eval_f1                 =     0.8385\n",
            "  eval_loss               =     1.0551\n",
            "  eval_runtime            = 0:00:05.58\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    132.263\n",
            "  eval_steps_per_second   =     16.645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "PaOa0hrXuE11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "j7gpN1inleRc",
        "outputId": "6d249eeb-85ed-455f-adc2-b1369090fa41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Create save path\n",
        "!mkdir -p /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/OneShot/1/\n",
        "## Copy saved model.\n",
        "!cp -r /content/models/OneShot/1/* /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/OneShot/1/"
      ],
      "metadata": {
        "id": "ieIK9gu_lfjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Bring back saved model here. \n",
        "#!mkdir -p /content/models/OneShot/1/\n",
        "# !cp -r /content/gdrive/MyDrive/ColabData/SemEval2022Task2/TaskA/OneShot/1/* /content/models/OneShot/1/"
      ],
      "metadata": {
        "id": "ODA2pT_PliHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on dev"
      ],
      "metadata": {
        "id": "saNaytr0uH6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eval on dev data\n",
        "!python /content/AStitchInLanguageModels/Dataset/Task2/Utils/run_glue_f1_macro.py \\\n",
        "    \t--model_name_or_path '/content/models/OneShot/1' \\\n",
        "    \t--do_predict \\\n",
        "    \t--max_seq_length 128 \\\n",
        "    \t--per_device_train_batch_size 32 \\\n",
        "    \t--learning_rate 2e-5 \\\n",
        "    \t--num_train_epochs 9 \\\n",
        "    \t--evaluation_strategy \"epoch\" \\\n",
        "    \t--output_dir models/OneShot/1/eval-dev/ \\\n",
        "    \t--seed 1 \\\n",
        "    \t--train_file      Data/OneShot/train.csv \\\n",
        "    \t--validation_file Data/OneShot/dev.csv \\\n",
        "      --test_file Data/OneShot/dev.csv \\\n",
        "\t    --evaluation_strategy \"epoch\" \\\n",
        "\t    --save_strategy \"epoch\"  \\\n",
        "\t    --load_best_model_at_end \\\n",
        "\t    --metric_for_best_model \"f1\" \\\n",
        "\t    --save_total_limit 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pw7X4NJlnpW",
        "outputId": "8cb2712c-d8fa-4080-b4a9-a30a2975595d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=epoch,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=models/OneShot/1/eval-dev/runs/Nov04_17-30-23_e5fee68aa035,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=f1,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=9.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=models/OneShot/1/eval-dev/,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=models/OneShot/1/eval-dev/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=1,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:__main__:load a local file for train: Data/OneShot/train.csv\n",
            "INFO:__main__:load a local file for validation: Data/OneShot/dev.csv\n",
            "INFO:__main__:load a local file for test: Data/OneShot/dev.csv\n",
            "WARNING:datasets.builder:Using custom data configuration default-3ad5094a97119ecc\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-3ad5094a97119ecc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "\rDownloading data files:   0% 0/3 [00:00<?, ?it/s]\rDownloading data files: 100% 3/3 [00:00<00:00, 9694.08it/s]\n",
            "\rExtracting data files:   0% 0/3 [00:00<?, ?it/s]\rExtracting data files: 100% 3/3 [00:00<00:00, 1481.74it/s]\n",
            "\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \r\r0 tables [00:00, ? tables/s]\r                            \rDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3ad5094a97119ecc/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "\r  0% 0/3 [00:00<?, ?it/s]\r100% 3/3 [00:00<00:00, 1041.29it/s]\n",
            "[INFO|configuration_utils.py:652] 2022-11-04 17:30:24,532 >> loading configuration file /content/models/OneShot/1/config.json\n",
            "[INFO|configuration_utils.py:706] 2022-11-04 17:30:24,538 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/models/OneShot/1\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.24.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 17:30:24,538 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 17:30:24,538 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 17:30:24,538 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 17:30:24,538 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1773] 2022-11-04 17:30:24,538 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:2155] 2022-11-04 17:30:24,679 >> loading weights file /content/models/OneShot/1/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2608] 2022-11-04 17:30:26,829 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[INFO|modeling_utils.py:2617] 2022-11-04 17:30:26,829 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at /content/models/OneShot/1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            " 80% 4/5 [00:00<00:00,  4.22ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]\n",
            "  0% 0/1 [00:00<?, ?ba/s]\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:726] 2022-11-04 17:30:30,466 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:30:30,468 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:30:30,468 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:30:30,468 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 18.20it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.839\n",
            "  eval_f1                 =     0.8385\n",
            "  eval_loss               =     1.0551\n",
            "  eval_runtime            = 0:00:06.31\n",
            "  eval_samples            =        739\n",
            "  eval_samples_per_second =    117.045\n",
            "  eval_steps_per_second   =      14.73\n",
            "INFO:__main__:*** Test ***\n",
            "[INFO|trainer.py:726] 2022-11-04 17:30:36,786 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1. If sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2929] 2022-11-04 17:30:36,787 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2931] 2022-11-04 17:30:36,787 >>   Num examples = 739\n",
            "[INFO|trainer.py:2934] 2022-11-04 17:30:36,787 >>   Batch size = 8\n",
            "100% 93/93 [00:05<00:00, 18.30it/s]\n",
            "INFO:__main__:***** Test results None *****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create submission file for dev data\n",
        "\n",
        "params = {\n",
        "    'submission_format_file' : '/content/outputs/zero_shot_dev_formated.csv' ,\n",
        "    'input_file'             : '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev.csv'                   ,\n",
        "    'prediction_format_file' : '/content/models/OneShot/1/eval-dev/test_results_None.txt'                        ,\n",
        "    }\n",
        "params[ 'setting' ] = 'one_shot'\n",
        "\n",
        "updated_data = insert_to_submission_file( **params )\n",
        "write_csv( updated_data, 'outputs/both_dev_formated.csv' ) \n",
        "\n",
        "# run eval script\n",
        "\n",
        "import sys\n",
        "sys.path.append( '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/' ) \n",
        "from SubTask1Evaluator import evaluate_submission\n",
        "\n",
        "\n",
        "submission_file = 'outputs/both_dev_formated.csv'\n",
        "gold_file       = '/content/SemEval_2022_Task2-idiomaticity/SubTaskA/Data/dev_gold.csv'\n",
        "\n",
        "results = evaluate_submission( submission_file, gold_file )\n",
        "%reload_ext google.colab.data_table\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data=results[1:], columns=results[0])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "yrRZjVPFlp2j",
        "outputId": "b95d36a9-f519-471b-941b-8d7994884ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote outputs/both_dev_formated.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Settings Languages  F1 Score (Macro)\n",
              "0  zero_shot        EN          0.718350\n",
              "1  zero_shot        PT          0.637363\n",
              "2  zero_shot     EN,PT          0.705875\n",
              "3   one_shot        EN          0.836850\n",
              "4   one_shot        PT          0.824373\n",
              "5   one_shot     EN,PT          0.838545"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a8e2eaa-9f33-4d01-9be1-ce8f023f6adb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Settings</th>\n",
              "      <th>Languages</th>\n",
              "      <th>F1 Score (Macro)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.718350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.637363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zero_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.705875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN</td>\n",
              "      <td>0.836850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>PT</td>\n",
              "      <td>0.824373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>one_shot</td>\n",
              "      <td>EN,PT</td>\n",
              "      <td>0.838545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a8e2eaa-9f33-4d01-9be1-ce8f023f6adb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a8e2eaa-9f33-4d01-9be1-ce8f023f6adb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a8e2eaa-9f33-4d01-9be1-ce8f023f6adb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a8bd4d5e58f96183/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"zero_shot\",\n\"EN\",\n{\n            'v': 0.7183498489001335,\n            'f': \"0.7183498489001335\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"zero_shot\",\n\"PT\",\n{\n            'v': 0.6373626373626374,\n            'f': \"0.6373626373626374\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"zero_shot\",\n\"EN,PT\",\n{\n            'v': 0.705875232245273,\n            'f': \"0.705875232245273\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"one_shot\",\n\"EN\",\n{\n            'v': 0.8368496021716297,\n            'f': \"0.8368496021716297\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"one_shot\",\n\"PT\",\n{\n            'v': 0.8243734616245245,\n            'f': \"0.8243734616245245\",\n        }],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"one_shot\",\n\"EN,PT\",\n{\n            'v': 0.8385446787729242,\n            'f': \"0.8385446787729242\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"string\", \"Settings\"], [\"string\", \"Languages\"], [\"number\", \"F1 Score (Macro)\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}